{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DistilBERT_Final (80%).ipynb","provenance":[{"file_id":"1nyXWsXsMO6zSVJtJZC0j0Dj59Ff9uMZ4","timestamp":1660187683744},{"file_id":"1_pLcmm1XMLexl5kQfpLMqSIu-v7_hfC0","timestamp":1660185387503},{"file_id":"1C6YYRRSsb7xMHVZe6B_N3y42ufYk1CwH","timestamp":1660099830014}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"1c98b962d624422ebbbd5baf43834162":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_07d619d80d7145d7ac7f04d1aa094ae9","IPY_MODEL_085c0d8245f640eea149e16a9932c869","IPY_MODEL_15a5806832bc4605b8cd49bd6315ff9a"],"layout":"IPY_MODEL_11147fb3a5184aa989ebe2a1b8d4eaf7"}},"07d619d80d7145d7ac7f04d1aa094ae9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c98aa7a1b3764d3e9cf8524b2b076819","placeholder":"​","style":"IPY_MODEL_7da4a223f8c040c38475d671b4f41a3b","value":"Downloading: 100%"}},"085c0d8245f640eea149e16a9932c869":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9bc170f08f35403098471d4e517e6003","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b49a70675a5343958fb7b5be41d3df36","value":231508}},"15a5806832bc4605b8cd49bd6315ff9a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d517b384aca34ac1bef86fae8bfde42a","placeholder":"​","style":"IPY_MODEL_274f9123a1fb4dc88a44119d1824976c","value":" 232k/232k [00:00&lt;00:00, 282kB/s]"}},"11147fb3a5184aa989ebe2a1b8d4eaf7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c98aa7a1b3764d3e9cf8524b2b076819":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7da4a223f8c040c38475d671b4f41a3b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9bc170f08f35403098471d4e517e6003":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b49a70675a5343958fb7b5be41d3df36":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d517b384aca34ac1bef86fae8bfde42a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"274f9123a1fb4dc88a44119d1824976c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1241d88ad60b44a0b36a21e389038c2f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7656be0612f8402da73c148ff415b8a4","IPY_MODEL_1aa028c5a72b4a17bf5ad9734ff22aef","IPY_MODEL_8926a38788634e26800046bdaf7ee047"],"layout":"IPY_MODEL_bd60d9b17a374ecfa409977712147dc2"}},"7656be0612f8402da73c148ff415b8a4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b85cc7013f544eefa72bb1408569e12d","placeholder":"​","style":"IPY_MODEL_3265f411c375440dad70536a295489e0","value":"Downloading: 100%"}},"1aa028c5a72b4a17bf5ad9734ff22aef":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b4b1087053ea4e628d0656fe15f2a8f5","max":483,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fb9bcc72d1904b6fb429501c04e1dd93","value":483}},"8926a38788634e26800046bdaf7ee047":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2396659e18a94c16a4003291f2d3dd09","placeholder":"​","style":"IPY_MODEL_f9ea665f054d44fa885711596f923d8a","value":" 483/483 [00:00&lt;00:00, 14.8kB/s]"}},"bd60d9b17a374ecfa409977712147dc2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b85cc7013f544eefa72bb1408569e12d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3265f411c375440dad70536a295489e0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b4b1087053ea4e628d0656fe15f2a8f5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fb9bcc72d1904b6fb429501c04e1dd93":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2396659e18a94c16a4003291f2d3dd09":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f9ea665f054d44fa885711596f923d8a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e3aa8db352e44568a4323289c0149cd5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_63d808e93df4486583f010bf6114764e","IPY_MODEL_2dd053dcad374ca0a5ecfa536fa5c3e6","IPY_MODEL_a9e772a7ae6a495384315afdf2d1ffc4"],"layout":"IPY_MODEL_8fbaf870991349828441b6e55087c29b"}},"63d808e93df4486583f010bf6114764e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0a358a82764a41c0b745be8c50013722","placeholder":"​","style":"IPY_MODEL_75ca7de65e9b4bac86a969e7e6fa94da","value":"Downloading: 100%"}},"2dd053dcad374ca0a5ecfa536fa5c3e6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c40f22ca67254cba99ddaed309ea4d10","max":267967963,"min":0,"orientation":"horizontal","style":"IPY_MODEL_641f1123bbfa4ceeaafe8305ad5b6e04","value":267967963}},"a9e772a7ae6a495384315afdf2d1ffc4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fdd40825dd1f4ae1a6d25578dbd3f23d","placeholder":"​","style":"IPY_MODEL_83164eb888504d6e8b43df99020b47f0","value":" 268M/268M [00:04&lt;00:00, 58.4MB/s]"}},"8fbaf870991349828441b6e55087c29b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0a358a82764a41c0b745be8c50013722":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"75ca7de65e9b4bac86a969e7e6fa94da":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c40f22ca67254cba99ddaed309ea4d10":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"641f1123bbfa4ceeaafe8305ad5b6e04":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fdd40825dd1f4ae1a6d25578dbd3f23d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"83164eb888504d6e8b43df99020b47f0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b86d333d6b2649998132ec5c555914ce":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1b89337fd3d14e06b82c5a967d4b3bd5","IPY_MODEL_afa245f3182d4310a9b4bc6e84979a36","IPY_MODEL_97116924e40e4686a6b90af7d9a52efa"],"layout":"IPY_MODEL_bce9c8c9753e42d595e6ec527da63540"}},"1b89337fd3d14e06b82c5a967d4b3bd5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a3ec9fd1455f403b856d6a6278655649","placeholder":"​","style":"IPY_MODEL_73c910f8b0754163b66096f4b3e0aed7","value":"Downloading: 100%"}},"afa245f3182d4310a9b4bc6e84979a36":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b5614c6a1e3a49798820a6ac0e7c7500","max":29,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a5477dd5d4db4eadab2cafc8d20c3514","value":29}},"97116924e40e4686a6b90af7d9a52efa":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_16bbc80bc3f74171bd568a7642bc39fe","placeholder":"​","style":"IPY_MODEL_dcccf38758484951b7d2f37e950f6060","value":" 29.0/29.0 [00:00&lt;00:00, 793B/s]"}},"bce9c8c9753e42d595e6ec527da63540":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a3ec9fd1455f403b856d6a6278655649":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"73c910f8b0754163b66096f4b3e0aed7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b5614c6a1e3a49798820a6ac0e7c7500":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a5477dd5d4db4eadab2cafc8d20c3514":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"16bbc80bc3f74171bd568a7642bc39fe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dcccf38758484951b7d2f37e950f6060":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"738278ba7b3d405cbb107b318828dd58":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_21df255357c14028a4dd25e0d97894a9","IPY_MODEL_ba6c310c69fb4ff6ae50432512ece7c2","IPY_MODEL_6e94b5077f5549ceb0a90771e59f69a8"],"layout":"IPY_MODEL_2c4005a9e691474c99bd145ba39fa513"}},"21df255357c14028a4dd25e0d97894a9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_36f0a5c95ed545f9bddfb0b11840080e","placeholder":"​","style":"IPY_MODEL_8d0ad30b0d1c4711bb0fa7de87673699","value":"Downloading: 100%"}},"ba6c310c69fb4ff6ae50432512ece7c2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_96edfc8254e14df08065b2588a439920","max":435797,"min":0,"orientation":"horizontal","style":"IPY_MODEL_dbc9a1fea6044ff5b4e95d875368be3c","value":435797}},"6e94b5077f5549ceb0a90771e59f69a8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c195fcbf907d479e827ee78efe6a8b12","placeholder":"​","style":"IPY_MODEL_ccd1b93c38594f37b15bf49f6083953a","value":" 436k/436k [00:01&lt;00:00, 489kB/s]"}},"2c4005a9e691474c99bd145ba39fa513":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"36f0a5c95ed545f9bddfb0b11840080e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8d0ad30b0d1c4711bb0fa7de87673699":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"96edfc8254e14df08065b2588a439920":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dbc9a1fea6044ff5b4e95d875368be3c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c195fcbf907d479e827ee78efe6a8b12":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ccd1b93c38594f37b15bf49f6083953a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"WqR9cK3zblzA"},"source":["# DISTILBERT"]},{"cell_type":"markdown","metadata":{"id":"9VjmKu_saHNG"},"source":["# 1.Load Dataset"]},{"cell_type":"markdown","source":["## 1.1 Import torch"],"metadata":{"id":"4ZBJN6Aar0Qe"}},{"cell_type":"code","source":["# Install necessary files\n","!pip install torch==1.4.0\n","!pip install sentencepiece\n","!pip install transformers==3.5.1\n","!pip install wget"],"metadata":{"id":"H1vuzcsS_dRM"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oYsV4H8fCpZ-","colab":{"base_uri":"https://localhost:8080/"},"outputId":"85347833-d045-4225-c4c5-e11f7aabc793","executionInfo":{"status":"ok","timestamp":1660319386294,"user_tz":240,"elapsed":1104,"user":{"displayName":"Soham Shinde","userId":"16480171809645537358"}}},"source":["# Instructing PyTorch to use the GPU.\n","import torch\n","\n","if torch.cuda.is_available():    \n","    device = torch.device(\"cuda\")\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","    print('Current GPU:', torch.cuda.get_device_name(0))\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")\n","\n","# Releases all unoccupied cached memory \n","torch.cuda.empty_cache()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["There are 1 GPU(s) available.\n","Current GPU: Tesla T4\n"]}]},{"cell_type":"markdown","metadata":{"id":"npF6oYe3I551"},"source":["## 1.2 Download Dataset"]},{"cell_type":"markdown","metadata":{"id":"08pO03Ff1BjI"},"source":["The dataset is hosted on GitHub in this repo: https://rajpurkar.github.io/SQuAD-explorer/"]},{"cell_type":"code","metadata":{"id":"pMtmPMkBzrvs","colab":{"base_uri":"https://localhost:8080/"},"outputId":"027a4b8a-0f20-4bad-a5ba-74f0795138de","executionInfo":{"status":"ok","timestamp":1660319395787,"user_tz":240,"elapsed":3789,"user":{"displayName":"Soham Shinde","userId":"16480171809645537358"}}},"source":["# The dataset is hosted on GitHub in this repo: https://rajpurkar.github.io/SQuAD-explorer/\n","import wget\n","import os\n","\n","# Setup local directory\n","print('Downloading dataset...')\n","local_dir = './squad_dataset/'\n","\n","# The filenames and URLs for the dataset files.\n","files = [('train-v1.1.json', 'https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json'), \n","         ('dev-v1.1.json', 'https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json'),\n","         ('evaluate-v1.1.py', 'https://github.com/allenai/bi-att-flow/blob/master/squad/evaluate-v1.1.py')]\n","\n","# Create directory if needed\n","if not os.path.exists(local_dir):\n","    os.mkdir(local_dir)\n","\n","# Download data-files\n","for (filename, url) in files:\n","    file_path = local_dir + filename\n","    if not os.path.exists(file_path):\n","        print('  ' + file_path)\n","        wget.download(url, local_dir + filename)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading dataset...\n","  ./squad_dataset/train-v1.1.json\n","  ./squad_dataset/dev-v1.1.json\n","  ./squad_dataset/evaluate-v1.1.py\n"]}]},{"cell_type":"code","metadata":{"id":"D87pElNqYqVu","colab":{"base_uri":"https://localhost:8080/"},"outputId":"59a5d13b-2e35-4955-cfb9-5fb4d5aaa0e6","executionInfo":{"status":"ok","timestamp":1660319395787,"user_tz":240,"elapsed":5,"user":{"displayName":"Soham Shinde","userId":"16480171809645537358"}}},"source":["# Printing file size and location in the drive.\n","data_dir = './squad_dataset/'\n","files = list(os.listdir(data_dir))\n","\n","print('Dataset Location:', data_dir)\n","for f in files:\n","    f_size = float(os.stat(data_dir + '/' + f).st_size) / 2**20\n","    print(\"     {:25s}    {:>6.2f} MB\".format(f, f_size))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset Location: ./squad_dataset/\n","     train-v1.1.json               28.89 MB\n","     dev-v1.1.json                  4.63 MB\n","     evaluate-v1.1.py               0.19 MB\n"]}]},{"cell_type":"markdown","metadata":{"id":"NFrrVK-HDOEj"},"source":["## 1.3 Parse Dataset"]},{"cell_type":"code","metadata":{"id":"Xm1wTn09RAR7"},"source":["# The SQuAD dataset is stored in 'json' format. \n","# There 87,599 training samples in the dataset.\n","import json\n","\n","with open(os.path.join('./squad_dataset/train-v1.1.json'), \"r\", encoding=\"utf-8\") as reader:\n","    input_data = json.load(reader)[\"data\"]\n","\n","# List of dictionary of each row\n","examples = []\n","\n","for entry in input_data:\n","    title = entry[\"title\"] # Extract the title\n","    # print('  ', title)\n","    for paragraph in entry[\"paragraphs\"]:\n","        context_text = paragraph[\"context\"] # Extract the context\n","        for qa in paragraph[\"qas\"]:\n","            # Store Question and answer data in dictionary\n","            ex = {}\n","            ex['qas_id'] = qa[\"id\"]\n","            ex['question_text'] = qa[\"question\"]\n","            answer = qa[\"answers\"][0]\n","            ex['answer_text'] = answer[\"text\"]\n","            ex['start_position_character'] = answer[\"answer_start\"]                \n","            ex['title'] = title\n","            ex['context_text'] = context_text\n","            examples.append(ex)\n","\n","# print('There are {:,} training examples.'.format(len(examples)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["examples[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KNL2s6W4u4Z9","executionInfo":{"status":"ok","timestamp":1660319399099,"user_tz":240,"elapsed":3,"user":{"displayName":"Soham Shinde","userId":"16480171809645537358"}},"outputId":"511bf80b-3b61-4314-ab7e-e9cc34a25079"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'answer_text': 'Saint Bernadette Soubirous',\n"," 'context_text': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n"," 'qas_id': '5733be284776f41900661182',\n"," 'question_text': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?',\n"," 'start_position_character': 515,\n"," 'title': 'University_of_Notre_Dame'}"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":["## 1.4 Inspecting Examples:"],"metadata":{"id":"u3LStms5t5G1"}},{"cell_type":"markdown","metadata":{"id":"W8OZ4ZWSClyd"},"source":["Each example has a **question**, and a **context**, which is the reference text in which the answer can be found. \n","\n","\n","Here are some of the field descriptions from the code:\n","* **qas_id**: The example's unique identifier\n","* **title**: Article title\n","* **question_text**: The question string\n","* **context_text**: The context string\n","* **answer_text**: The answer string\n"]},{"cell_type":"code","metadata":{"id":"zIC3pmmgIHjk","colab":{"base_uri":"https://localhost:8080/"},"outputId":"fdf4f947-f9ec-41bc-d3fb-110cba743236","executionInfo":{"status":"ok","timestamp":1660319401782,"user_tz":240,"elapsed":330,"user":{"displayName":"Soham Shinde","userId":"16480171809645537358"}}},"source":["import textwrap\n","\n","wrapper = textwrap.TextWrapper(width=80) \n","ex = examples[260]\n","print('Title:', ex['title'])\n","print('ID:', ex['qas_id'])\n","\n","print('\\n======== Question =========')\n","print(ex['question_text'])\n","\n","print('\\n======== Context =========')\n","print(wrapper.fill(ex['context_text']))\n","\n","print('\\n======== Answer =========')\n","print(ex['answer_text'])\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Title: University_of_Notre_Dame\n","ID: 5733ccbe4776f41900661271\n","\n","======== Question =========\n","In what film did a parody of the \"Win one for the Gipper\" speech appear?\n","\n","======== Context =========\n","In the film Knute Rockne, All American, Knute Rockne (played by Pat O'Brien)\n","delivers the famous \"Win one for the Gipper\" speech, at which point the\n","background music swells with the \"Notre Dame Victory March\". George Gipp was\n","played by Ronald Reagan, whose nickname \"The Gipper\" was derived from this role.\n","This scene was parodied in the movie Airplane! with the same background music,\n","only this time honoring George Zipp, one of Ted Striker's former comrades. The\n","song also was prominent in the movie Rudy, with Sean Astin as Daniel \"Rudy\"\n","Ruettiger, who harbored dreams of playing football at the University of Notre\n","Dame despite significant obstacles.\n","\n","======== Answer =========\n","Airplane!\n"]}]},{"cell_type":"markdown","metadata":{"id":"csoyhyZzSSg_"},"source":["## 1.5 Helper Functions"]},{"cell_type":"code","metadata":{"id":"gpt6tR83keZD"},"source":["import time\n","import datetime\n","\n","# Helper function for formatting elapsed times.\n","# Converts floating point seconds into hh:mm:ss\n","def format_time(elapsed):\n","    '''\n","    Takes a time in seconds and returns a string hh:mm:ss\n","    '''\n","    elapsed_rounded = int(round((elapsed)))\n","    return str(datetime.timedelta(seconds=elapsed_rounded))\n","\n","# Helper function to automatically pick a reasonable interval for printing out a progress update during training.\n","# For printing updates, this will choose an interval.\n","def good_update_interval(total_iters, num_desired_updates):\n","    '''\n","    Progress update interval based on the magnitude of the total iterations.\n","    Parameters:\n","      `total_iters` - The number of iterations in the for-loop.\n","      `num_desired_updates` - How many times we want to see an update over the \n","                              course of the for-loop.\n","    '''\n","    exact_interval = total_iters / num_desired_updates\n","    order_of_mag = len(str(total_iters)) - 1\n","    round_mag = order_of_mag - 1\n","    update_interval = int(round(exact_interval, -round_mag))\n","    if update_interval == 0:\n","        update_interval = 1\n","    return update_interval\n","\n","import pandas as pd\n","import csv\n","\n","# Helper function to report current GPU memory usage.\n","# Reports how much of the GPU's memory we're using.\n","def check_gpu_mem():\n","    '''\n","    Uses Nvidia's SMI tool to check the current GPU memory usage.\n","    '''\n","    buf = os.popen('nvidia-smi --query-gpu=memory.total,memory.used --format=csv')\n","    reader = csv.reader(buf, delimiter=',')\n","    df = pd.DataFrame(reader)\n","    new_header = df.iloc[0]\n","    df = df[1:]\n","    df.columns = new_header\n","    return df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hk4InpYWDWWi"},"source":["# 2.Data Preprocessing\n","\n","\n"]},{"cell_type":"markdown","source":["## 2.1 Import Tokenizer"],"metadata":{"id":"jSbP0Qfh0o7T"}},{"cell_type":"code","source":["# a row in the dataframe\n","examples[260]"],"metadata":{"id":"VvMwF6EBbjvt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1660319407397,"user_tz":240,"elapsed":333,"user":{"displayName":"Soham Shinde","userId":"16480171809645537358"}},"outputId":"94140562-fd19-49a8-ebdb-44c9a2243eba"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'answer_text': 'Airplane!',\n"," 'context_text': 'In the film Knute Rockne, All American, Knute Rockne (played by Pat O\\'Brien) delivers the famous \"Win one for the Gipper\" speech, at which point the background music swells with the \"Notre Dame Victory March\". George Gipp was played by Ronald Reagan, whose nickname \"The Gipper\" was derived from this role. This scene was parodied in the movie Airplane! with the same background music, only this time honoring George Zipp, one of Ted Striker\\'s former comrades. The song also was prominent in the movie Rudy, with Sean Astin as Daniel \"Rudy\" Ruettiger, who harbored dreams of playing football at the University of Notre Dame despite significant obstacles.',\n"," 'qas_id': '5733ccbe4776f41900661271',\n"," 'question_text': 'In what film did a parody of the \"Win one for the Gipper\" speech appear?',\n"," 'start_position_character': 344,\n"," 'title': 'University_of_Notre_Dame'}"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"znwmOxQsl9fE","colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["1c98b962d624422ebbbd5baf43834162","07d619d80d7145d7ac7f04d1aa094ae9","085c0d8245f640eea149e16a9932c869","15a5806832bc4605b8cd49bd6315ff9a","11147fb3a5184aa989ebe2a1b8d4eaf7","c98aa7a1b3764d3e9cf8524b2b076819","7da4a223f8c040c38475d671b4f41a3b","9bc170f08f35403098471d4e517e6003","b49a70675a5343958fb7b5be41d3df36","d517b384aca34ac1bef86fae8bfde42a","274f9123a1fb4dc88a44119d1824976c"]},"executionInfo":{"status":"ok","timestamp":1660319418006,"user_tz":240,"elapsed":9387,"user":{"displayName":"Soham Shinde","userId":"16480171809645537358"}},"outputId":"18714c02-0552-4460-f59e-3f9f2159c400"},"source":["# Importing the tokenizer\n","from transformers import DistilBertTokenizer\n","tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c98b962d624422ebbbd5baf43834162"}},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"zhrb-u7G-yzW"},"source":["# Distributing Sequence Length \n","# Choosing max_len\n","max_len = 384"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u0GHfndc7FlH"},"source":["## 2.2 Tokenizing the training set"]},{"cell_type":"code","metadata":{"id":"_ucdyc5z7TmO","colab":{"base_uri":"https://localhost:8080/"},"outputId":"24e6a350-f9ea-496e-8398-50a1e497ab9f","executionInfo":{"status":"ok","timestamp":1660319740044,"user_tz":240,"elapsed":319611,"user":{"displayName":"Soham Shinde","userId":"16480171809645537358"}}},"source":["import torch\n","\n","# Time\n","t0 = time.time()\n","\n","# Lists\n","all_input_ids = []\n","attention_masks = []\n","segment_ids = [] \n","start_positions = []\n","end_positions = []\n","\n","num_dropped = 0\n","\n","# for Update-Interval\n","update_interval = good_update_interval(total_iters = len(examples), num_desired_updates = 15)\n","\n","print('Tokenizing {:,} examples...'.format(len(examples)))\n","\n","for (ex_num, ex) in enumerate(examples):\n","    # Display update information\n","    if (ex_num % update_interval) == 0 and not (ex_num == 0):\n","        elapsed = format_time(time.time() - t0)\n","        ex_per_sec = (time.time() - t0) / ex_num\n","        remaining_sec = ex_per_sec * (len(examples) - ex_num)\n","        remaining = format_time(remaining_sec)\n","        print('  Example {:>7,}  of  {:>7,}.    Elapsed: {:}. Remaining: {:}'.format(ex_num, len(examples), elapsed, remaining))\n","    \n","    answer_tokens = tokenizer.tokenize(ex['answer_text']) # Tokenize the answer\n","    sentinel_str = ' '.join(['[MASK]']*len(answer_tokens)) # \"[MASK] [MASK] [MASK] [MASK] [MASK]\"\n","    start_char_i = ex['start_position_character']\n","    end_char_i = start_char_i + len(ex['answer_text']) # Compute position of end character\n","    context_w_sentinel = ex['context_text'][:start_char_i] + sentinel_str + ex['context_text'][end_char_i:] # context-string with sentinel_str in position of answer\n","    \n","    # Returns a dictionary containing the encoded sequence or sequence pair and additional information: the mask for sequence classification and the overflowing elements if a max_length is specified.\n","    encoded_dict = tokenizer.encode_plus(\n","        ex['question_text'], \n","        context_w_sentinel,\n","        add_special_tokens = True,\n","        max_length = max_len,\n","        pad_to_max_length = True,\n","        truncation = True,\n","        return_attention_mask = True,\n","        return_tensors = 'pt')\n","    \n","    # They are token indices, numerical representations of tokens building the sequences that will be used as input by the model.\n","    input_ids = encoded_dict['input_ids']\n","\n","    # A special token representing a masked token (used by masked-language modeling pretraining objectives, like BERT).\n","    is_mask_token = (input_ids[0] == tokenizer.mask_token_id)\n","    \n","    mask_token_indices = is_mask_token.nonzero(as_tuple=False)[:, 0]\n","    if not len(mask_token_indices) == len(answer_tokens):\n","        num_dropped += 1\n","        continue\n","    \n","    start_index = mask_token_indices[0]\n","    end_index = mask_token_indices[-1]\n","    \n","    # Converts a string to a sequence of ids (integer), using the tokenizer and vocabulary.\n","    # Returns The tokenized ids of the text.\n","    answer_token_ids = tokenizer.encode(answer_tokens, \n","                                        add_special_tokens=False, \n","                                        return_tensors='pt') # Return Pytorch model\n","    \n","\n","    input_ids[0, start_index : end_index + 1] = answer_token_ids\n","    \n","    all_input_ids.append(input_ids)\n","    attention_masks.append(encoded_dict['attention_mask'])    \n","    #segment_ids.append(encoded_dict['token_type_ids'])\n","    start_positions.append(start_index)\n","    end_positions.append(end_index)\n","\n","# Concatenates the given sequence of seq tensors in the given dimension. All tensors must either have the same shape (except in the concatenating dimension) or be empty.\n","all_input_ids = torch.cat(all_input_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","#segment_ids = torch.cat(segment_ids, dim=0)\n","# Constructs a tensor with no autograd history by copying data\n","start_positions = torch.tensor(start_positions)\n","end_positions = torch.tensor(end_positions)\n","\n","print('DONE.  Tokenization took {:}'.format(format_time(time.time() - t0)))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenizing 87,599 examples...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["  Example   6,000  of   87,599.    Elapsed: 0:00:24. Remaining: 0:05:21\n","  Example  12,000  of   87,599.    Elapsed: 0:00:43. Remaining: 0:04:29\n","  Example  18,000  of   87,599.    Elapsed: 0:00:59. Remaining: 0:03:50\n","  Example  24,000  of   87,599.    Elapsed: 0:01:18. Remaining: 0:03:26\n","  Example  30,000  of   87,599.    Elapsed: 0:01:39. Remaining: 0:03:10\n","  Example  36,000  of   87,599.    Elapsed: 0:02:02. Remaining: 0:02:55\n","  Example  42,000  of   87,599.    Elapsed: 0:02:25. Remaining: 0:02:38\n","  Example  48,000  of   87,599.    Elapsed: 0:02:49. Remaining: 0:02:19\n","  Example  54,000  of   87,599.    Elapsed: 0:03:12. Remaining: 0:01:59\n","  Example  60,000  of   87,599.    Elapsed: 0:03:34. Remaining: 0:01:38\n","  Example  66,000  of   87,599.    Elapsed: 0:03:56. Remaining: 0:01:17\n","  Example  72,000  of   87,599.    Elapsed: 0:04:19. Remaining: 0:00:56\n","  Example  78,000  of   87,599.    Elapsed: 0:04:41. Remaining: 0:00:35\n","  Example  84,000  of   87,599.    Elapsed: 0:05:05. Remaining: 0:00:13\n","DONE.  Tokenization took 0:05:19\n"]}]},{"cell_type":"markdown","metadata":{"id":"Wl12-BD637Ty"},"source":["# 3.Fine-Tuning BERT"]},{"cell_type":"markdown","metadata":{"id":"Lw6mvOX8ELxB"},"source":["## 3.1 Loading Initial Weights"]},{"cell_type":"code","metadata":{"id":"gFsCTp_mporB","colab":{"base_uri":"https://localhost:8080/","height":190,"referenced_widgets":["1241d88ad60b44a0b36a21e389038c2f","7656be0612f8402da73c148ff415b8a4","1aa028c5a72b4a17bf5ad9734ff22aef","8926a38788634e26800046bdaf7ee047","bd60d9b17a374ecfa409977712147dc2","b85cc7013f544eefa72bb1408569e12d","3265f411c375440dad70536a295489e0","b4b1087053ea4e628d0656fe15f2a8f5","fb9bcc72d1904b6fb429501c04e1dd93","2396659e18a94c16a4003291f2d3dd09","f9ea665f054d44fa885711596f923d8a","e3aa8db352e44568a4323289c0149cd5","63d808e93df4486583f010bf6114764e","2dd053dcad374ca0a5ecfa536fa5c3e6","a9e772a7ae6a495384315afdf2d1ffc4","8fbaf870991349828441b6e55087c29b","0a358a82764a41c0b745be8c50013722","75ca7de65e9b4bac86a969e7e6fa94da","c40f22ca67254cba99ddaed309ea4d10","641f1123bbfa4ceeaafe8305ad5b6e04","fdd40825dd1f4ae1a6d25578dbd3f23d","83164eb888504d6e8b43df99020b47f0"]},"executionInfo":{"status":"ok","timestamp":1660319758838,"user_tz":240,"elapsed":12864,"user":{"displayName":"Soham Shinde","userId":"16480171809645537358"}},"outputId":"a178156a-5429-4a8a-f453-0f52f607aec9"},"source":["# The AlbertForQuestionAnswering class from the transformers library can be used for this project.\n","from transformers import DistilBertForQuestionAnswering\n","model = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-uncased', output_attentions = False, output_hidden_states = False)\n","\n","desc = model.cuda() # .cuda() Function Can Only Specify GPU."],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/483 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1241d88ad60b44a0b36a21e389038c2f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/268M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3aa8db352e44568a4323289c0149cd5"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForQuestionAnswering: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"markdown","metadata":{"id":"aRp4O7D295d_"},"source":["## 3.2 Sampling and Validation Set\n"]},{"cell_type":"code","metadata":{"id":"GEgLpFVlo1Z-","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a14b307a-0bf2-40ed-882c-e1ec70130213","executionInfo":{"status":"ok","timestamp":1660319767547,"user_tz":240,"elapsed":718,"user":{"displayName":"Soham Shinde","userId":"16480171809645537358"}}},"source":["# Represents a Python iterable over a dataset\n","from torch.utils.data import TensorDataset # Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension.\n","import numpy as np\n","\n","subsample = True\n","if subsample:\n","  # Randomly permute a sequence\n","    all_indices = np.random.permutation(all_input_ids.shape[0])\n","    indices = all_indices[0:87000]\n","    dataset = TensorDataset(all_input_ids[indices, :], \n","                            attention_masks[indices, :], \n","                            #segment_ids[indices, :], \n","                            start_positions[indices], \n","                            end_positions[indices])\n","else:\n","    dataset = TensorDataset(all_input_ids, \n","                            attention_masks, \n","                            #segment_ids, \n","                            start_positions, \n","                            end_positions)\n","    \n","print('Dataset size: {:} samples'.format(len(dataset)))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset size: 87000 samples\n"]}]},{"cell_type":"code","metadata":{"id":"7a6HUzC8wJTQ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"55a134dd-2b11-4e12-b246-188132ad434b","executionInfo":{"status":"ok","timestamp":1660319769746,"user_tz":240,"elapsed":349,"user":{"displayName":"Soham Shinde","userId":"16480171809645537358"}}},"source":["#This dataset already has a train / test split, but I'm dividing this training set to use 98% for training and 2% for validation\n","\n","from torch.utils.data import random_split\n","\n","train_size = int(0.98 * len(dataset))\n","val_size = len(dataset) - train_size\n","\n","train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n","\n","print('{:>5,} training samples'.format(train_size))\n","print('{:>5,} validation samples'.format(val_size))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["85,260 training samples\n","1,740 validation samples\n"]}]},{"cell_type":"markdown","metadata":{"id":"ejHQThm_uVnB"},"source":["## 3.3 Batch Size and DataLoaders"]},{"cell_type":"code","metadata":{"id":"XGUqOCtgqGhP","colab":{"base_uri":"https://localhost:8080/"},"outputId":"835c3b31-f75d-4bec-a759-8986fc2f2774","executionInfo":{"status":"ok","timestamp":1660319770283,"user_tz":240,"elapsed":2,"user":{"displayName":"Soham Shinde","userId":"16480171809645537358"}}},"source":["from torch.utils.data import DataLoader # Iterable Constructor \n","from torch.utils.data import RandomSampler # Samples elements randomly\n","from torch.utils.data import SubsetRandomSampler # Samples elements randomly from a given list of indices, without replacement\n","from torch.utils.data import SequentialSampler # Samples elements sequentially, always in the same order\n","\n","import numpy.random\n","import numpy as np\n","\n","batch_size = 12 \n","train_dataloader = DataLoader(\n","            train_dataset,\n","            sampler = RandomSampler(train_dataset),\n","            batch_size = batch_size\n","        )\n","validation_dataloader = DataLoader(\n","            val_dataset,\n","            sampler = SequentialSampler(val_dataset),\n","            batch_size = batch_size\n","        )\n","print('{:,} training batches & {:,} validation batches'.format(len(train_dataloader), len(validation_dataloader)))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["7,105 training batches & 145 validation batches\n"]}]},{"cell_type":"markdown","metadata":{"id":"8o-VEBobKwHk"},"source":["**Optimizer:**\n","For the purposes of fine-tuning, the authors recommend choosing from the following values:\n","- Learning rate (Adam): 5e-5, 3e-5, 2e-5\n","- Number of epochs: 2, 3, 4"]},{"cell_type":"code","metadata":{"id":"GLs72DuMODJO"},"source":["from transformers import AdamW\n","optimizer = AdamW(model.parameters(), lr = 3e-5, eps = 1e-8)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_iaG0A5quuqz"},"source":["## 3.4 Epochs and Learning Rate Scheduler"]},{"cell_type":"code","metadata":{"id":"-p0upAhhRiIx"},"source":["# Create a schedule with a learning rate that decreases linearly from the initial lr set in the optimizer to 0, \n","# after a warmup period during which it increases linearly from 0 to the initial lr set in the optimizer.\n","from transformers import get_linear_schedule_with_warmup\n","\n","epochs = 2\n","\n","total_steps = len(train_dataloader) * epochs\n","\n","scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                            num_warmup_steps = 0, # Default value in run_glue.py\n","                                            num_training_steps = total_steps)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s-m54ne8uMmi","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9fb02b26-b8fa-49bb-c863-4d11b0a1f7b7","executionInfo":{"status":"ok","timestamp":1660319775435,"user_tz":240,"elapsed":2,"user":{"displayName":"Soham Shinde","userId":"16480171809645537358"}}},"source":["print('Total number of steps: {}'.format(total_steps))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total number of steps: 14210\n"]}]},{"cell_type":"markdown","metadata":{"id":"RqfmWwUR_Sox"},"source":["## 3.5 Training Loop"]},{"cell_type":"code","metadata":{"id":"WiDKq4cLQG6H","colab":{"base_uri":"https://localhost:8080/"},"outputId":"33f4102b-9e26-47bf-90b0-38c75f6effef","executionInfo":{"status":"ok","timestamp":1660326433724,"user_tz":240,"elapsed":6656755,"user":{"displayName":"Soham Shinde","userId":"16480171809645537358"}}},"source":["import random\n","import numpy as np\n","\n","seed_val = 42\n","\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","training_stats = []\n","\n","for epoch_i in range(0, epochs):\n","\n","    print(\"\")\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    print('Training {:,} batches...'.format(len(train_dataloader)))\n","\n","    t0 = time.time()\n","    total_train_loss = 0\n","    model.train()\n","\n","    # Setup the update interval\n","    update_interval = good_update_interval(\n","                total_iters = len(train_dataloader), \n","                num_desired_updates = 15\n","            )\n","\n","    num_batches = len(train_dataloader)\n","\n","    # iterate through each batch\n","    for step, batch in enumerate(train_dataloader):\n","        # Display the update interval\n","        if step % update_interval == 0 and not step == 0:\n","            elapsed = format_time(time.time() - t0)\n","            step_per_sec = (time.time() - t0) / step\n","            remaining_sec = step_per_sec * (num_batches - step)\n","            remaining = format_time(remaining_sec)\n","            print('  Batch {:>7,}  of  {:>7,}.    Elapsed: {:}. Remaining: {:}'.format(step, num_batches, elapsed, remaining))\n","\n","        # moves the model to the device\n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        #b_seg_ids = batch[2].to(device)\n","        b_start_pos = batch[2].to(device)\n","        b_end_pos = batch[3].to(device)\n","\n","        # Sets the gradients of all optimized torch.Tensor s to zero\n","        model.zero_grad()\n","\n","        # Ouput\n","        outputs = model(b_input_ids, \n","                        attention_mask=b_input_mask, \n","                        #token_type_ids = b_seg_ids,\n","                        start_positions=b_start_pos,\n","                        end_positions=b_end_pos)\n","       \n","        # Output Tuple ( Total span extraction loss is the sum of a Cross-Entropy for the start and end positions, Span-start scores (before SoftMax) , Span-end scores (before SoftMax))\n","        (loss, start_logits, end_logits) = outputs\n","\n","        total_train_loss += loss.item() # Returns the value of this tensor as a standard Python number. This only works for tensors with one element. For other cases, see tolist().\n","        loss.backward() # Computes the gradient of current tensor w.r.t. graph leaves.\n","        \n","        # Clips gradient norm of an iterable of parameters.\n","        # The norm is computed over all gradients together, as if they were concatenated into a single vector. Gradients are modified in-place.\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) \n","\n","        optimizer.step() # method that updates the parameters\n","        scheduler.step()\n","    \n","    # END OF INNER FOR LOOP .........................................................................................\n","\n","\n","    avg_train_loss = total_train_loss / len(train_dataloader)            \n","    \n","    training_time = format_time(time.time() - t0)\n","\n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","    print(\"  Training epoch took: {:}\".format(training_time))\n","        \n","    print(\"\")\n","    print(\"Running Validation...\")\n","\n","    # In addition, the common practice for evaluating/validation is using torch.no_grad() in pair with model.eval() to turn off gradients computation\n","    model.eval()\n","\n","    total_eval_accuracy = 0\n","    total_eval_loss = 0\n","\n","    t0_val = time.time()\n","    pred_start, pred_end, true_start, true_end = [], [], [], []\n","\n","    # Compute Validation Metrics\n","    for batch in validation_dataloader:\n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        #b_seg_ids = batch[2].to(device)\n","        b_start_pos = batch[2].to(device)\n","        b_end_pos = batch[3].to(device)\n","        with torch.no_grad():\n","            outputs = model(b_input_ids, \n","                            #token_type_ids=b_seg_ids, \n","                            attention_mask=b_input_mask,\n","                            start_positions=b_start_pos,\n","                            end_positions=b_end_pos)\n","\n","        (loss, start_logits, end_logits) = outputs        \n","\n","        total_eval_loss += loss.item()\n","        start_logits = start_logits.detach().cpu().numpy()\n","        end_logits = end_logits.detach().cpu().numpy()\n","      \n","        b_start_pos = b_start_pos.to('cpu').numpy()\n","        b_end_pos = b_end_pos.to('cpu').numpy()\n","\n","        answer_start = np.argmax(start_logits, axis=1)\n","        answer_end = np.argmax(end_logits, axis=1)\n","\n","        pred_start.append(answer_start)\n","        pred_end.append(answer_end)\n","        true_start.append(b_start_pos)\n","        true_end.append(b_end_pos)\n","\n","    pred_start = np.concatenate(pred_start, axis=0)\n","    pred_end = np.concatenate(pred_end, axis=0)\n","    true_start = np.concatenate(true_start, axis=0)\n","    true_end = np.concatenate(true_end, axis=0)\n","\n","    num_start_correct = np.sum(pred_start == true_start)\n","    num_end_correct = np.sum(pred_end == true_end)\n","\n","    total_correct = num_start_correct + num_end_correct\n","    total_indices = len(true_start) + len(true_end)\n","\n","    avg_val_accuracy = float(total_correct) / float(total_indices)\n","    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n","\n","    avg_val_loss = total_eval_loss / len(validation_dataloader)\n","    \n","    validation_time = format_time(time.time() - t0_val)\n","    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n","    print(\"  Validation took: {:}\".format(validation_time))\n","\n","    training_stats.append(\n","        {\n","            'epoch': epoch_i + 1,\n","            'Training Loss': avg_train_loss,\n","            'Valid. Loss': avg_val_loss,\n","            'Valid. Accur.': avg_val_accuracy,\n","            'Training Time': training_time,\n","            'Validation Time': validation_time\n","        }\n","    )\n","\n","print(\"\")\n","print(\"Training complete!\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","======== Epoch 1 / 2 ========\n","Training 7,105 batches...\n","  Batch     500  of    7,105.    Elapsed: 0:03:42. Remaining: 0:48:49\n","  Batch   1,000  of    7,105.    Elapsed: 0:07:35. Remaining: 0:46:15\n","  Batch   1,500  of    7,105.    Elapsed: 0:11:28. Remaining: 0:42:50\n","  Batch   2,000  of    7,105.    Elapsed: 0:15:21. Remaining: 0:39:10\n","  Batch   2,500  of    7,105.    Elapsed: 0:19:13. Remaining: 0:35:24\n","  Batch   3,000  of    7,105.    Elapsed: 0:23:06. Remaining: 0:31:36\n","  Batch   3,500  of    7,105.    Elapsed: 0:26:59. Remaining: 0:27:48\n","  Batch   4,000  of    7,105.    Elapsed: 0:30:52. Remaining: 0:23:58\n","  Batch   4,500  of    7,105.    Elapsed: 0:34:45. Remaining: 0:20:07\n","  Batch   5,000  of    7,105.    Elapsed: 0:38:38. Remaining: 0:16:16\n","  Batch   5,500  of    7,105.    Elapsed: 0:42:32. Remaining: 0:12:25\n","  Batch   6,000  of    7,105.    Elapsed: 0:46:25. Remaining: 0:08:33\n","  Batch   6,500  of    7,105.    Elapsed: 0:50:18. Remaining: 0:04:41\n","  Batch   7,000  of    7,105.    Elapsed: 0:54:11. Remaining: 0:00:49\n","\n","  Average training loss: 1.43\n","  Training epoch took: 0:55:00\n","\n","Running Validation...\n","  Accuracy: 0.70\n","  Validation Loss: 1.06\n","  Validation took: 0:00:22\n","\n","======== Epoch 2 / 2 ========\n","Training 7,105 batches...\n","  Batch     500  of    7,105.    Elapsed: 0:03:53. Remaining: 0:51:17\n","  Batch   1,000  of    7,105.    Elapsed: 0:07:46. Remaining: 0:47:24\n","  Batch   1,500  of    7,105.    Elapsed: 0:11:39. Remaining: 0:43:33\n","  Batch   2,000  of    7,105.    Elapsed: 0:15:33. Remaining: 0:39:41\n","  Batch   2,500  of    7,105.    Elapsed: 0:19:26. Remaining: 0:35:47\n","  Batch   3,000  of    7,105.    Elapsed: 0:23:19. Remaining: 0:31:54\n","  Batch   3,500  of    7,105.    Elapsed: 0:27:12. Remaining: 0:28:01\n","  Batch   4,000  of    7,105.    Elapsed: 0:31:05. Remaining: 0:24:07\n","  Batch   4,500  of    7,105.    Elapsed: 0:34:57. Remaining: 0:20:14\n","  Batch   5,000  of    7,105.    Elapsed: 0:38:51. Remaining: 0:16:21\n","  Batch   5,500  of    7,105.    Elapsed: 0:42:44. Remaining: 0:12:28\n","  Batch   6,000  of    7,105.    Elapsed: 0:46:37. Remaining: 0:08:35\n","  Batch   6,500  of    7,105.    Elapsed: 0:50:30. Remaining: 0:04:42\n","  Batch   7,000  of    7,105.    Elapsed: 0:54:23. Remaining: 0:00:49\n","\n","  Average training loss: 0.86\n","  Training epoch took: 0:55:12\n","\n","Running Validation...\n","  Accuracy: 0.71\n","  Validation Loss: 1.03\n","  Validation took: 0:00:22\n","\n","Training complete!\n"]}]},{"cell_type":"markdown","source":["## 3.6 Save and Load Model"],"metadata":{"id":"vWQKO6d59dNZ"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GThzfEmpyNMt","executionInfo":{"status":"ok","timestamp":1660326641383,"user_tz":240,"elapsed":31197,"user":{"displayName":"Soham Shinde","userId":"16480171809645537358"}},"outputId":"2490f105-734f-483a-d09a-f1f84ef8e31d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import pickle"],"metadata":{"id":"hqzEI8Duy5Sz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pickle.dump(model, open('/content/drive/MyDrive/NLP/distil_model.pkl', 'wb'))"],"metadata":{"id":"KmI22bhSyoas"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#pickled_model = pickle.load(open('/content/drive/MyDrive/NLP/albert_model.pkl', 'rb'))"],"metadata":{"id":"vgqnvb-tyfBF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_xgvwYTHCrs4"},"source":["## 3.7 Training Results"]},{"cell_type":"code","metadata":{"id":"6O_NbXFGMukX","colab":{"base_uri":"https://localhost:8080/","height":143},"outputId":"b2a6c872-f3ad-4cac-f3af-bfad6fb3f826","executionInfo":{"status":"ok","timestamp":1660326663313,"user_tz":240,"elapsed":317,"user":{"displayName":"Soham Shinde","userId":"16480171809645537358"}}},"source":["# Checking for Over-Fitting\n","import pandas as pd\n","\n","df_stats = pd.DataFrame(data=training_stats)\n","df_stats = df_stats.set_index('epoch')\n","pd.set_option('precision', 2)\n","df_stats"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n","epoch                                                                         \n","1               1.43         1.06           0.70       0:55:00         0:00:22\n","2               0.86         1.03           0.71       0:55:12         0:00:22"],"text/html":["\n","  <div id=\"df-4e64a7e0-7ac0-4ab1-853f-0a97f7536b09\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Training Loss</th>\n","      <th>Valid. Loss</th>\n","      <th>Valid. Accur.</th>\n","      <th>Training Time</th>\n","      <th>Validation Time</th>\n","    </tr>\n","    <tr>\n","      <th>epoch</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>1.43</td>\n","      <td>1.06</td>\n","      <td>0.70</td>\n","      <td>0:55:00</td>\n","      <td>0:00:22</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.86</td>\n","      <td>1.03</td>\n","      <td>0.71</td>\n","      <td>0:55:12</td>\n","      <td>0:00:22</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4e64a7e0-7ac0-4ab1-853f-0a97f7536b09')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-4e64a7e0-7ac0-4ab1-853f-0a97f7536b09 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-4e64a7e0-7ac0-4ab1-853f-0a97f7536b09');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":24}]},{"cell_type":"markdown","metadata":{"id":"b-M5ABsu6KX6"},"source":["# 4.Performance On Test Set"]},{"cell_type":"markdown","source":["## 4.1 Load Trained & Pre-tuned Model"],"metadata":{"id":"mzFArLXq_Qse"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"k0kXfNzS_dVJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1660326672343,"user_tz":240,"elapsed":2442,"user":{"displayName":"Soham Shinde","userId":"16480171809645537358"}},"outputId":"ca63e3f9-28b2-4c6d-e728-2dfc889065c4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["import pickle\n","pickled_model = pickle.load(open('/content/drive/MyDrive/NLP/distil_model.pkl', 'rb'))\n","model = pickled_model"],"metadata":{"id":"32k8ECN6-lc0"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Iw7CrBfSWuFi","colab":{"base_uri":"https://localhost:8080/","height":454,"referenced_widgets":["b86d333d6b2649998132ec5c555914ce","1b89337fd3d14e06b82c5a967d4b3bd5","afa245f3182d4310a9b4bc6e84979a36","97116924e40e4686a6b90af7d9a52efa","bce9c8c9753e42d595e6ec527da63540","a3ec9fd1455f403b856d6a6278655649","73c910f8b0754163b66096f4b3e0aed7","b5614c6a1e3a49798820a6ac0e7c7500","a5477dd5d4db4eadab2cafc8d20c3514","16bbc80bc3f74171bd568a7642bc39fe","dcccf38758484951b7d2f37e950f6060","738278ba7b3d405cbb107b318828dd58","21df255357c14028a4dd25e0d97894a9","ba6c310c69fb4ff6ae50432512ece7c2","6e94b5077f5549ceb0a90771e59f69a8","2c4005a9e691474c99bd145ba39fa513","36f0a5c95ed545f9bddfb0b11840080e","8d0ad30b0d1c4711bb0fa7de87673699","96edfc8254e14df08065b2588a439920","dbc9a1fea6044ff5b4e95d875368be3c","c195fcbf907d479e827ee78efe6a8b12","ccd1b93c38594f37b15bf49f6083953a"]},"executionInfo":{"status":"error","timestamp":1660327816996,"user_tz":240,"elapsed":8671,"user":{"displayName":"Soham Shinde","userId":"16480171809645537358"}},"outputId":"e1623001-8b89-44cf-9ee7-9ab1c64143d4"},"source":["# Loading the holdout dataset and testing the fine-tuned BERT's performance.\n","from transformers import AlbertTokenizer, AlbertForQuestionAnswering\n","\n","pre_tuned = True\n","\n","if pre_tuned:\n","    tokenizer = AlbertTokenizer.from_pretrained(\"distilbert-base-cased-distilled-squad\")\n","    model = AlbertForQuestionAnswering.from_pretrained(\"distilbert-base-cased-distilled-squad\")\n","    desc = model.cuda()\n"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b86d333d6b2649998132ec5c555914ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/436k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"738278ba7b3d405cbb107b318828dd58"}},"metadata":{}},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-48-cd28617f582d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpre_tuned\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAlbertTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"distilbert-base-cased-distilled-squad\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAlbertForQuestionAnswering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"distilbert-base-cased-distilled-squad\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mdesc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m         return cls._from_pretrained(\n\u001b[0;32m-> 1653\u001b[0;31m             \u001b[0mresolved_vocab_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_configuration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minit_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1654\u001b[0m         )\n\u001b[1;32m   1655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1723\u001b[0m         \u001b[0;31m# Instantiate tokenizer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1724\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1725\u001b[0;31m             \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minit_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1726\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1727\u001b[0m             raise OSError(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/tokenization_albert.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vocab_file, do_lower_case, remove_space, keep_accents, bos_token, eos_token, unk_token, sep_token, pad_token, cls_token, mask_token, **kwargs)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msp_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentencePieceProcessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msp_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sentencepiece.py\u001b[0m in \u001b[0;36mLoad\u001b[0;34m(self, model_file, model_proto)\u001b[0m\n\u001b[1;32m    365\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mmodel_proto\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoadFromSerializedProto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_proto\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoadFromFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sentencepiece.py\u001b[0m in \u001b[0;36mLoadFromFile\u001b[0;34m(self, arg)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mLoadFromFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_sentencepiece\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentencePieceProcessor_LoadFromFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     def Init(self,\n","\u001b[0;31mTypeError\u001b[0m: not a string"]}]},{"cell_type":"markdown","metadata":{"id":"fiIjg4LEpPfe"},"source":["## 4.2 Parsing Test Set"]},{"cell_type":"markdown","metadata":{"id":"vrfD3PzMGsTm"},"source":["There are **3 answers** provided for every question. These are three human-provided answers, and they don't always agree. For example, for the question:\n","\n","```\n","Where did Super Bowl 50 take place?\n","```\n","\n","The annotators produced:\n","```\n","   {'answer_start': 403, 'text': 'Santa Clara, California'}\n","   {'answer_start': 355, 'text': \"Levi's Stadium\"}\n","   {'answer_start': 355, 'text': \"Levi's Stadium in the San Francisco Bay Area at Santa Clara, California.\"}\n","```\n","\n","Since all three seem acceptable, BERT's prediction to all three correct answers are compared, and  the highest F1 score that BERT gets among the three is considered."]},{"cell_type":"code","metadata":{"id":"gS9qRVcHGsTn","colab":{"base_uri":"https://localhost:8080/"},"outputId":"5aa60537-c46f-455e-e022-80c25fe060d5","executionInfo":{"status":"ok","timestamp":1660327818913,"user_tz":240,"elapsed":337,"user":{"displayName":"Soham Shinde","userId":"16480171809645537358"}}},"source":["import json\n","\n","with open(os.path.join('./squad_dataset/dev-v1.1.json'), \"r\", encoding=\"utf-8\") as reader: input_data = json.load(reader)[\"data\"]\n","\n","print_count = 0\n","#print('Unpacking SQuAD Examples...')\n","#print('Articles:')\n","\n","examples = []\n","for entry in input_data:\n","    title = entry[\"title\"]\n","    #print('  ', title)\n","    for paragraph in entry[\"paragraphs\"]:\n","        context_text = paragraph[\"context\"]\n","        for qa in paragraph[\"qas\"]:\n","            ex = {}\n","            ex['qas_id'] = qa[\"id\"]\n","            ex['question_text'] = qa[\"question\"]\n","            ex['answers'] = qa[\"answers\"]\n","            ex['title'] = title\n","            ex['context_text'] = context_text\n","            examples.append(ex)\n","print('DONE!')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["DONE!\n"]}]},{"cell_type":"code","source":["examples[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CwAi-dKUvK6e","executionInfo":{"status":"ok","timestamp":1660327821300,"user_tz":240,"elapsed":353,"user":{"displayName":"Soham Shinde","userId":"16480171809645537358"}},"outputId":"67913574-448e-43b3-f575-60f6244be7f6"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'answers': [{'answer_start': 177, 'text': 'Denver Broncos'},\n","  {'answer_start': 177, 'text': 'Denver Broncos'},\n","  {'answer_start': 177, 'text': 'Denver Broncos'}],\n"," 'context_text': 'Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.',\n"," 'qas_id': '56be4db0acb8001400a502ec',\n"," 'question_text': 'Which NFL team represented the AFC at Super Bowl 50?',\n"," 'title': 'Super_Bowl_50'}"]},"metadata":{},"execution_count":50}]},{"cell_type":"code","metadata":{"id":"P4VnY9cPGsTq","colab":{"base_uri":"https://localhost:8080/"},"outputId":"696ba514-0912-42b3-c33a-c9e26ebc59a7","executionInfo":{"status":"ok","timestamp":1660327823149,"user_tz":240,"elapsed":2,"user":{"displayName":"Soham Shinde","userId":"16480171809645537358"}}},"source":["print('There are {:,} test examples.'.format(len(examples)))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["There are 10,570 test examples.\n"]}]},{"cell_type":"markdown","metadata":{"id":"LBV0qHNaNT7R"},"source":["## 4.3 Locating Test Answers"]},{"cell_type":"markdown","metadata":{"id":"W6t4DoepSgtZ"},"source":["For the test samples, a 2-pass approach has been followed to tokenize the dataset. \n","\n","In the first pass,all the samples are tokenized **without any truncation or padding**, which allows us to correctly locate the answers, even if their token indices are greater than 384.\n","\n","In the second pass, the samples are tokenized and encoded , with padding and truncation.\n"]},{"cell_type":"code","metadata":{"id":"bCVlKb7zoRMK","colab":{"base_uri":"https://localhost:8080/"},"outputId":"73b09e4a-ae5f-4efb-b497-c6f63e5622a4","executionInfo":{"status":"ok","timestamp":1660327947978,"user_tz":240,"elapsed":122579,"user":{"displayName":"Soham Shinde","userId":"16480171809645537358"}}},"source":["import time\n","import torch\n","import logging\n","logging.getLogger(\"transformers.tokenization_utils_base\").setLevel(logging.ERROR)\n","\n","t0 = time.time()\n","\n","start_positions = []\n","end_positions = []\n","num_clipped_answers = 0\n","num_impossible = 0\n","\n","# Set the update Interval\n","update_interval = good_update_interval(\n","            total_iters = len(examples), \n","            num_desired_updates = 15)\n","\n","print('Processing {:,} examples...'.format(len(examples)))\n","\n","for (ex_num, ex) in enumerate(examples):\n","\n","    # Display update-interval information\n","    if (ex_num % update_interval) == 0 and not (ex_num == 0):\n","        elapsed = format_time(time.time() - t0)\n","        ex_per_sec = (time.time() - t0) / ex_num\n","        remaining_sec = ex_per_sec * (len(examples) - ex_num)\n","        remaining = format_time(remaining_sec)\n","        print('  Example {:>7,}  of  {:>7,}.    Elapsed: {:}. Remaining: {:}'.format(ex_num, len(examples), elapsed, remaining))\n","\n","\n","    start_options = []\n","    end_options = []\n","\n","    encoded_stored = False\n","\n","    for answer in ex['answers']:\n","        answer_tokens = tokenizer.tokenize(answer['text'])\n","        sentinel_str = ' '.join(['[MASK]']*len(answer_tokens))\n","        start_char_i = answer['answer_start']\n","        end_char_i = start_char_i + len(answer['text'])\n","        context_w_sentinel = ex['context_text'][:start_char_i] + sentinel_str + ex['context_text'][end_char_i:]\n","        input_ids = tokenizer.encode(\n","            ex['question_text'], \n","            context_w_sentinel,\n","            add_special_tokens = True, \n","            #max_length = max_len,\n","            pad_to_max_length = False,\n","            truncation = False)\n","        \n","        mask_token_indices = np.where(np.array(input_ids) == tokenizer.mask_token_id)[0]\n","        assert(len(mask_token_indices) == len(answer_tokens))           \n","        start_index = mask_token_indices[0]\n","        end_index = mask_token_indices[-1]\n","        start_options.append(start_index)\n","        end_options.append(end_index)\n","    \n","    start_positions.append(start_options)\n","    end_positions.append(end_options)\n","\n","print('DONE.  Tokenization took {:}'.format(format_time(time.time() - t0)))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Processing 10,570 examples...\n","  Example   1,000  of   10,570.    Elapsed: 0:00:09. Remaining: 0:01:29\n","  Example   2,000  of   10,570.    Elapsed: 0:00:17. Remaining: 0:01:15\n","  Example   3,000  of   10,570.    Elapsed: 0:00:28. Remaining: 0:01:10\n","  Example   4,000  of   10,570.    Elapsed: 0:00:41. Remaining: 0:01:08\n","  Example   5,000  of   10,570.    Elapsed: 0:00:55. Remaining: 0:01:02\n","  Example   6,000  of   10,570.    Elapsed: 0:01:07. Remaining: 0:00:51\n","  Example   7,000  of   10,570.    Elapsed: 0:01:19. Remaining: 0:00:40\n","  Example   8,000  of   10,570.    Elapsed: 0:01:30. Remaining: 0:00:29\n","  Example   9,000  of   10,570.    Elapsed: 0:01:41. Remaining: 0:00:18\n","  Example  10,000  of   10,570.    Elapsed: 0:01:53. Remaining: 0:00:06\n","DONE.  Tokenization took 0:02:02\n"]}]},{"cell_type":"markdown","metadata":{"id":"P2cNKUUdStRt"},"source":["Effect of truncation strategy on the  test set questions.\n","\n"]},{"cell_type":"code","metadata":{"id":"5W6fhTTMrM4-","colab":{"base_uri":"https://localhost:8080/"},"outputId":"83903fe2-7dd6-458f-a7b8-badaa916b07f","executionInfo":{"status":"ok","timestamp":1660327951244,"user_tz":240,"elapsed":1074,"user":{"displayName":"Soham Shinde","userId":"16480171809645537358"}}},"source":["num_impossible = 0\n","num_clipped = 0\n","\n","for (start_options, end_options) in zip(start_positions, end_positions):\n","\n","    is_possible = False\n","    for i in range(0, len(start_options)):\n","        if (start_options[i] < max_len) and (end_options[i] < max_len):\n","            is_possible = True\n","        if (start_options[i] > max_len) or (end_options[i] > max_len):\n","            num_clipped += 1\n","    if not is_possible:\n","        num_impossible += 1\n","\n","print('')\n","\n","print('Samples w/ all answers clipped: {:,} of {:,} ({:.2%})'.format(num_impossible, len(examples), float(num_impossible) / float(len(examples))))\n","\n","addtl_clipped = num_clipped - (num_impossible * 3)\n","total_answers = len(examples) * 3\n","print('\\n    Additional clipped answers: {:,} of {:,}'.format(addtl_clipped, total_answers))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Samples w/ all answers clipped: 28 of 10,570 (0.26%)\n","\n","    Additional clipped answers: 20 of 31,710\n"]}]},{"cell_type":"markdown","metadata":{"id":"0RvMIcRhrCxS"},"source":["## 4.4 Tokenizing and Encoding the  Test Samples"]},{"cell_type":"code","metadata":{"id":"UXmTMK9nqX9B","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8696dda2-80e7-486e-9988-3f4c881b619c","executionInfo":{"status":"ok","timestamp":1660327991142,"user_tz":240,"elapsed":37955,"user":{"displayName":"Soham Shinde","userId":"16480171809645537358"}}},"source":["import time\n","import torch\n","\n","t0 = time.time()\n","all_input_ids = []\n","attention_masks = []\n","#segment_ids = [] \n","\n","update_interval = good_update_interval(\n","            total_iters = len(examples), \n","            num_desired_updates = 15\n","        )\n","\n","print('Tokenizing {:,} examples...'.format(len(examples)))\n","\n","for (ex_num, ex) in enumerate(examples):\n","\n","    if (ex_num % update_interval) == 0 and not (ex_num == 0):\n","        elapsed = format_time(time.time() - t0)\n","        ex_per_sec = (time.time() - t0) / ex_num\n","        remaining_sec = ex_per_sec * (len(examples) - ex_num)\n","        remaining = format_time(remaining_sec)\n","        print('  Example {:>7,}  of  {:>7,}.    Elapsed: {:}. Remaining: {:}'.format(ex_num, len(examples), elapsed, remaining))\n","\n","    encoded_dict = tokenizer.encode_plus(\n","        ex['question_text'], \n","        ex['context_text'],\n","        add_special_tokens = True,\n","        max_length = max_len,\n","        pad_to_max_length = True,\n","        truncation = True,\n","        return_attention_mask = True,\n","        return_tensors = 'pt',\n","    )\n","    input_ids = encoded_dict['input_ids']\n"," \n","    all_input_ids.append(input_ids)\n","    attention_masks.append(encoded_dict['attention_mask'])    \n","    #segment_ids.append(encoded_dict['token_type_ids'])\n","all_input_ids = torch.cat(all_input_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","#segment_ids = torch.cat(segment_ids, dim=0)\n","\n","print('DONE.  Tokenization took {:}'.format(format_time(time.time() - t0)))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenizing 10,570 examples...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["  Example   1,000  of   10,570.    Elapsed: 0:00:03. Remaining: 0:00:29\n","  Example   2,000  of   10,570.    Elapsed: 0:00:06. Remaining: 0:00:25\n","  Example   3,000  of   10,570.    Elapsed: 0:00:09. Remaining: 0:00:22\n","  Example   4,000  of   10,570.    Elapsed: 0:00:14. Remaining: 0:00:22\n","  Example   5,000  of   10,570.    Elapsed: 0:00:18. Remaining: 0:00:20\n","  Example   6,000  of   10,570.    Elapsed: 0:00:22. Remaining: 0:00:16\n","  Example   7,000  of   10,570.    Elapsed: 0:00:25. Remaining: 0:00:13\n","  Example   8,000  of   10,570.    Elapsed: 0:00:29. Remaining: 0:00:09\n","  Example   9,000  of   10,570.    Elapsed: 0:00:32. Remaining: 0:00:06\n","  Example  10,000  of   10,570.    Elapsed: 0:00:36. Remaining: 0:00:02\n","DONE.  Tokenization took 0:00:38\n"]}]},{"cell_type":"markdown","metadata":{"id":"RJJVmwstqzEn"},"source":["\n","## 4.5 Evaluate On Test Set"]},{"cell_type":"code","metadata":{"id":"xqNzZkUx6KYL","colab":{"base_uri":"https://localhost:8080/"},"outputId":"df3026c8-634d-490f-8834-84834a8610a4","executionInfo":{"status":"ok","timestamp":1660328125248,"user_tz":240,"elapsed":134110,"user":{"displayName":"Soham Shinde","userId":"16480171809645537358"}}},"source":["import time\n","import numpy as np\n","\n","model.eval()\n","\n","t0 = time.time()\n","pred_start = []\n","pred_end = []\n","num_test_samples = all_input_ids.shape[0]\n","batch_size = 16\n","\n","num_batches = int(np.ceil(num_test_samples / batch_size))\n","\n","print('Evaluating on {:,} test batches...'.format(num_batches))\n","\n","batch_num = 0\n","for start_i in range(0, num_test_samples, batch_size):\n","    if ((batch_num % 50) == 0) and not (batch_num == 0):\n","      elapsed = format_time(time.time() - t0)\n","      batches_per_sec = (time.time() - t0) / batch_num\n","      remaining_sec = batches_per_sec * (num_batches - batch_num)\n","      remaining = format_time(remaining_sec)\n","      print('  Batch {:>7,}  of  {:>7,}.    Elapsed: {:}. Remaining: {:}'.format(batch_num, num_batches, elapsed, remaining))\n","\n","    end_i = min(start_i + batch_size, num_test_samples)\n","    b_input_ids = all_input_ids[start_i:end_i, :]\n","    b_attn_masks = attention_masks[start_i:end_i, :]\n","    #b_seg_ids = segment_ids[start_i:end_i, :]   \n","\n","    b_input_ids = b_input_ids.to(device)\n","    b_attn_masks = b_attn_masks.to(device)\n","    #b_seg_ids = b_seg_ids.to(device)\n","\n","    with torch.no_grad():\n","        (start_logits, end_logits) = model(b_input_ids, \n","                                           attention_mask=b_attn_masks)\n","                                           #token_type_ids=b_seg_ids)\n","    start_logits = start_logits.detach().cpu().numpy()\n","    end_logits = end_logits.detach().cpu().numpy()\n","\n","    answer_start = np.argmax(start_logits, axis=1)\n","    answer_end = np.argmax(end_logits, axis=1)\n","\n","    pred_start.append(answer_start)\n","    pred_end.append(answer_end)\n","\n","    batch_num += 1\n","\n","pred_start = np.concatenate(pred_start, axis=0)\n","pred_end = np.concatenate(pred_end, axis=0)\n","\n","print('    DONE.')\n","\n","print('\\nEvaluation took {:.0f} seconds.'.format(time.time() - t0))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Evaluating on 661 test batches...\n","  Batch      50  of      661.    Elapsed: 0:00:10. Remaining: 0:01:57\n","  Batch     100  of      661.    Elapsed: 0:00:19. Remaining: 0:01:49\n","  Batch     150  of      661.    Elapsed: 0:00:30. Remaining: 0:01:41\n","  Batch     200  of      661.    Elapsed: 0:00:40. Remaining: 0:01:33\n","  Batch     250  of      661.    Elapsed: 0:00:51. Remaining: 0:01:24\n","  Batch     300  of      661.    Elapsed: 0:01:01. Remaining: 0:01:14\n","  Batch     350  of      661.    Elapsed: 0:01:11. Remaining: 0:01:03\n","  Batch     400  of      661.    Elapsed: 0:01:21. Remaining: 0:00:53\n","  Batch     450  of      661.    Elapsed: 0:01:31. Remaining: 0:00:43\n","  Batch     500  of      661.    Elapsed: 0:01:41. Remaining: 0:00:33\n","  Batch     550  of      661.    Elapsed: 0:01:51. Remaining: 0:00:22\n","  Batch     600  of      661.    Elapsed: 0:02:02. Remaining: 0:00:12\n","  Batch     650  of      661.    Elapsed: 0:02:12. Remaining: 0:00:02\n","    DONE.\n","\n","Evaluation took 134 seconds.\n"]}]},{"cell_type":"markdown","metadata":{"id":"xKoBf7Tm4Ync"},"source":["#5.Results"]},{"cell_type":"markdown","metadata":{"id":"abzMLjfzEjng"},"source":["There are two standard approaches to scoring results on the SQuAD benchmark:\n","\n","1. Exact Match\n","2. F1 Score"]},{"cell_type":"markdown","metadata":{"id":"XYV1A4nIE5Q8"},"source":["**Exact Match**\n","\n","For this metric, the number of predicted start indices that are equal to the correct ones are added up. It is done for  the end indices as well, such that there are actually two \"points\" for every sample.\n","\n","To handle the 3 possible answers, we score our predictions against each of the answers separately, and select the  answer which best matches our prediction. So for each test sample, the highest possible score is 2. "]},{"cell_type":"code","metadata":{"id":"5sLFKdVV9Ifw","colab":{"base_uri":"https://localhost:8080/"},"outputId":"93960626-87ee-4706-87ed-74ca6c9244e1","executionInfo":{"status":"ok","timestamp":1660328153574,"user_tz":240,"elapsed":373,"user":{"displayName":"Soham Shinde","userId":"16480171809645537358"}}},"source":["total_correct = 0\n","\n","for i in range(0, len(pred_start)):\n","\n","    match_options = []\n","    for j in range (0, len(start_positions[i])):\n","        matches = 0\n","        if pred_start[i] == start_positions[i][j]:\n","            matches += 1\n","        if pred_end[i] == end_positions[i][j]:\n","            matches += 1\n","\n","        match_options.append(matches)\n","\n","    total_correct += (max(match_options))\n","total_indices = len(pred_start) + len(pred_end)\n","\n","print('Correctly predicted indeces: {:,} of {:,} ({:.2%})'.format(\n","    total_correct,\n","    total_indices,\n","    float(total_correct) / float(total_indices)\n","))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Correctly predicted indeces: 17,058 of 21,140 (80.69%)\n"]}]},{"cell_type":"markdown","metadata":{"id":"FCO7vPcP8GgE"},"source":["**F1 Score**\n","\n","The F1 score gives our model credit for predicting a span which partially intersects the correct one.\n","\n","*Formula :* \n","```python\n","    precision = 1.0 * num_same / len(pred_toks)\n","    recall = 1.0 * num_same / len(gold_toks)\n","    f1 = (2 * precision * recall) / (precision + recall)\n","```\n","To handle the 3 possible answers, F1 score for each sample is calculated separately and the one with the highest score is considered. \n","\n","The final F1 score is determined by taking the average over all the test samples.\n"]},{"cell_type":"code","metadata":{"id":"t9EzmbnI8LRt","colab":{"base_uri":"https://localhost:8080/"},"outputId":"19363b34-39a8-4b5b-9f63-7c53e89157e7","executionInfo":{"status":"ok","timestamp":1660328158503,"user_tz":240,"elapsed":365,"user":{"displayName":"Soham Shinde","userId":"16480171809645537358"}}},"source":["f1s = []\n","for i in range(0, len(pred_start)):\n","    pred_span = set(range(pred_start[i], pred_end[i] + 1))\n","    f1_options = []\n","    for j in range (0, len(start_positions[i])):\n","        true_span = set(range(start_positions[i][j], end_positions[i][j] + 1))    \n","        num_same = len(pred_span.intersection(true_span))\n","        if num_same == 0:\n","            f1_options.append(0)\n","            continue\n","        precision = float(num_same) / float(len(pred_span))\n","        recall = float(num_same) / float(len(true_span))\n","        f1 = (2 * precision * recall) / (precision + recall)\n","        f1_options.append(f1)\n","    f1s.append(max(f1_options))\n","\n","print('Average F1 Score: {:.3f}'.format(np.mean(f1s)))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Average F1 Score: 0.827\n"]}]},{"cell_type":"markdown","metadata":{"id":"yNryoCi9JWng"},"source":["**Final Score of our fine tuned DistilBERT base model:**\n","\n","```\n","Correctly predicted indeces: 17,058 of 21,140 (80.69%)\n","\n","Average F1 Score: 0.827\n","```"]},{"cell_type":"markdown","source":["**Final Score of Pre-tuned ALBERT base model:**\n","\n","```\n","Correctly predicted indeces: 15,845 of 21,140 (74.95%)\n","\n","Average F1 Score: 0.789\n","```"],"metadata":{"id":"di1u43CsFt-F"}}]}