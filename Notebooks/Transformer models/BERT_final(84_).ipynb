{"cells":[{"cell_type":"markdown","metadata":{"id":"WqR9cK3zblzA"},"source":["# BERT MODEL - Final"]},{"cell_type":"markdown","metadata":{"id":"9VjmKu_saHNG"},"source":["# 1.Load Dataset"]},{"cell_type":"markdown","metadata":{"id":"GI0iOY8zvZzL"},"source":["## 1.1 Import Torch"]},{"cell_type":"markdown","metadata":{"id":"cqG7FzRVFEIv"},"source":["Instructing PyTorch to use the GPU."]},{"cell_type":"code","source":["# Install necessary files\n","!pip install torch==1.4.0\n","!pip install sentencepiece\n","!pip install transformers==3.5.1\n","!pip install wget"],"metadata":{"id":"xX2-7oQFFPLx"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oYsV4H8fCpZ-","outputId":"ce67eab7-2404-4256-aa33-77af46bda560","executionInfo":{"status":"ok","timestamp":1660444183360,"user_tz":240,"elapsed":742,"user":{"displayName":"Soham","userId":"11550519457178793252"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["There are 1 GPU(s) available.\n","Current GPU: Tesla T4\n"]}],"source":["# Instructing PyTorch to use the GPU.\n","import torch\n","\n","if torch.cuda.is_available():    \n","    device = torch.device(\"cuda\")\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","    print('Current GPU:', torch.cuda.get_device_name(0))\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")\n","\n","# Releases all unoccupied cached memory \n","torch.cuda.empty_cache()"]},{"cell_type":"markdown","metadata":{"id":"2ElsnSNUridI"},"source":["## 1.2 Download Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pMtmPMkBzrvs","outputId":"1d9ebb2f-9179-4bd1-8f65-4cdea8202bf7","executionInfo":{"status":"ok","timestamp":1660444229679,"user_tz":240,"elapsed":712,"user":{"displayName":"Soham","userId":"11550519457178793252"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading dataset...\n","  ./squad_dataset/train-v1.1.json\n","  ./squad_dataset/dev-v1.1.json\n","  ./squad_dataset/evaluate-v1.1.py\n","Done!\n"]}],"source":["# The dataset source: https://rajpurkar.github.io/SQuAD-explorer/\n","import wget\n","import os\n","\n","# Setup local directory\n","print('Downloading dataset...')\n","local_dir = './squad_dataset/'\n","\n","# The filenames and URLs for the dataset files.\n","files = [('train-v1.1.json', 'https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json'), \n","         ('dev-v1.1.json', 'https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json'),\n","         ('evaluate-v1.1.py', 'https://github.com/allenai/bi-att-flow/blob/master/squad/evaluate-v1.1.py')]\n","\n","# Create directory if needed\n","if not os.path.exists(local_dir):\n","    os.mkdir(local_dir)\n","\n","# Download data-files\n","for (filename, url) in files:\n","    file_path = local_dir + filename\n","    if not os.path.exists(file_path):\n","        print('  ' + file_path)\n","        wget.download(url, local_dir + filename)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D87pElNqYqVu","outputId":"0e718567-f0d0-436b-a987-61e66ebea6de","executionInfo":{"status":"ok","timestamp":1660444232249,"user_tz":240,"elapsed":174,"user":{"displayName":"Soham","userId":"11550519457178793252"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset Location:\n","./squad_dataset/\n","     train-v1.1.json               28.89 MB\n","     evaluate-v1.1.py               0.19 MB\n","     dev-v1.1.json                  4.63 MB\n"]}],"source":["# Printing file size and location in the drive.\n","data_dir = './squad_dataset/'\n","files = list(os.listdir(data_dir))\n","\n","print('Dataset Location:', data_dir)\n","for f in files:\n","    f_size = float(os.stat(data_dir + '/' + f).st_size) / 2**20\n","    print(\"     {:25s}    {:>6.2f} MB\".format(f, f_size))"]},{"cell_type":"markdown","metadata":{"id":"NFrrVK-HDOEj"},"source":["## 1.3 Parse Dataset"]},{"cell_type":"markdown","metadata":{"id":"6VyKjSdzMGw6"},"source":["The SQuAD dataset is stored in 'json' format. There 87,599 training samples in the dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xm1wTn09RAR7","colab":{"base_uri":"https://localhost:8080/"},"outputId":"009007d1-0424-42de-db4e-abde2b982b3a","executionInfo":{"status":"ok","timestamp":1660444441947,"user_tz":240,"elapsed":977,"user":{"displayName":"Soham","userId":"11550519457178793252"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["   University_of_Notre_Dame\n","   Beyoncé\n","   Montana\n","   Genocide\n","   Antibiotics\n","   Frédéric_Chopin\n","   Sino-Tibetan_relations_during_the_Ming_dynasty\n","   IPod\n","   The_Legend_of_Zelda:_Twilight_Princess\n","   Spectre_(2015_film)\n","   2008_Sichuan_earthquake\n","   New_York_City\n","   To_Kill_a_Mockingbird\n","   Solar_energy\n","   Tajikistan\n","   Anthropology\n","   Portugal\n","   Kanye_West\n","   Buddhism\n","   American_Idol\n","   Dog\n","   2008_Summer_Olympics_torch_relay\n","   Alfred_North_Whitehead\n","   Financial_crisis_of_2007%E2%80%9308\n","   Saint_Barth%C3%A9lemy\n","   Genome\n","   Comprehensive_school\n","   Republic_of_the_Congo\n","   Prime_minister\n","   Institute_of_technology\n","   Wayback_Machine\n","   Dutch_Republic\n","   Symbiosis\n","   Canadian_Armed_Forces\n","   Cardinal_(Catholicism)\n","   Iranian_languages\n","   Lighting\n","   Separation_of_powers_under_the_United_States_Constitution\n","   Architecture\n","   Human_Development_Index\n","   Southern_Europe\n","   BBC_Television\n","   Arnold_Schwarzenegger\n","   Plymouth\n","   Heresy\n","   Warsaw_Pact\n","   Materialism\n","   Space_Race\n","   Pub\n","   Christian\n","   Sony_Music_Entertainment\n","   Oklahoma_City\n","   Hunter-gatherer\n","   United_Nations_Population_Fund\n","   Russian_Soviet_Federative_Socialist_Republic\n","   Universal_Studios\n","   Alexander_Graham_Bell\n","   Internet_service_provider\n","   Comics\n","   Saint_Helena\n","   Aspirated_consonant\n","   Hydrogen\n","   Web_browser\n","   Boston\n","   BeiDou_Navigation_Satellite_System\n","   Canon_law\n","   Communications_in_Somalia\n","   Catalan_language\n","   Estonian_language\n","   Paper\n","   Arena_Football_League\n","   Adult_contemporary_music\n","   Matter\n","   Westminster_Abbey\n","   Nanjing\n","   Bern\n","   Daylight_saving_time\n","   Royal_Institute_of_British_Architects\n","   National_Archives_and_Records_Administration\n","   Tristan_da_Cunha\n","   University_of_Kansas\n","   Political_corruption\n","   Dialect\n","   Classical_music\n","   Slavs\n","   Southampton\n","   Treaty\n","   Josip_Broz_Tito\n","   Marshall_Islands\n","   Szlachta\n","   Virgil\n","   Alps\n","   Gene\n","   Guinea-Bissau\n","   List_of_numbered_streets_in_Manhattan\n","   Brain\n","   Near_East\n","   Zhejiang\n","   Ministry_of_Defence_(United_Kingdom)\n","   High-definition_television\n","   Wood\n","   Somalis\n","   Middle_Ages\n","   Phonology\n","   Computer\n","   Black_people\n","   The_Times\n","   New_Delhi\n","   Imamah_(Shia_doctrine)\n","   Bird_migration\n","   Atlantic_City,_New_Jersey\n","   Immunology\n","   MP3\n","   House_music\n","   Letter_case\n","   Chihuahua_(state)\n","   Pitch_(music)\n","   England_national_football_team\n","   Houston\n","   Copper\n","   Identity_(social_science)\n","   Himachal_Pradesh\n","   Communication\n","   Grape\n","   Computer_security\n","   Orthodox_Judaism\n","   Animal\n","   Beer\n","   Race_and_ethnicity_in_the_United_States_Census\n","   United_States_dollar\n","   Imperial_College_London\n","   Gymnastics\n","   Hanover\n","   Emotion\n","   FC_Barcelona\n","   Everton_F.C.\n","   Old_English\n","   Aircraft_carrier\n","   Federal_Aviation_Administration\n","   Lancashire\n","   Mesozoic\n","   Videoconferencing\n","   Gregorian_calendar\n","   Xbox_360\n","   Military_history_of_the_United_States\n","   Hard_rock\n","   Great_Plains\n","   Infrared\n","   Biodiversity\n","   ASCII\n","   Digestion\n","   Federal_Bureau_of_Investigation\n","   Adolescence\n","   Antarctica\n","   Mary_(mother_of_Jesus)\n","   Melbourne\n","   John,_King_of_England\n","   Macintosh\n","   Anti-aircraft_warfare\n","   Sanskrit\n","   Valencia\n","   General_Electric\n","   United_States_Army\n","   Franco-Prussian_War\n","   Eritrea\n","   Uranium\n","   Order_of_the_British_Empire\n","   Age_of_Enlightenment\n","   Circadian_rhythm\n","   Elizabeth_II\n","   Sexual_orientation\n","   Dell\n","   Capital_punishment_in_the_United_States\n","   Nintendo_Entertainment_System\n","   Ashkenazi_Jews\n","   Athanasius_of_Alexandria\n","   Seattle\n","   Memory\n","   Multiracial_American\n","   Pharmaceutical_industry\n","   Umayyad_Caliphate\n","   Asphalt\n","   Queen_Victoria\n","   Freemasonry\n","   Israel\n","   Hellenistic_period\n","   Napoleon\n","   Bill_%26_Melinda_Gates_Foundation\n","   Northwestern_University\n","   Hokkien\n","   Montevideo\n","   Poultry\n","   Arsenal_F.C.\n","   Dutch_language\n","   Buckingham_Palace\n","   Incandescent_light_bulb\n","   Clothing\n","   Chicago_Cubs\n","   States_of_Germany\n","   Korean_War\n","   Royal_Dutch_Shell\n","   Copyright_infringement\n","   Greece\n","   Mammal\n","   East_India_Company\n","   Southeast_Asia\n","   Professional_wrestling\n","   Film_speed\n","   Mexico_City\n","   Germans\n","   New_Haven,_Connecticut\n","   Brigham_Young_University\n","   Myocardial_infarction\n","   Department_store\n","   Intellectual_property\n","   Florida\n","   Queen_(band)\n","   Presbyterianism\n","   Thuringia\n","   Predation\n","   Marvel_Comics\n","   British_Empire\n","   Botany\n","   Madonna_(entertainer)\n","   London\n","   Law_of_the_United_States\n","   Myanmar\n","   Jews\n","   Cotton\n","   Data_compression\n","   The_Sun_(United_Kingdom)\n","   Carnival\n","   Pesticide\n","   Somerset\n","   Yale_University\n","   Late_Middle_Ages\n","   Ann_Arbor,_Michigan\n","   Gothic_architecture\n","   Cubism\n","   Political_philosophy\n","   Alloy\n","   Norfolk_Island\n","   Edmund_Burke\n","   Samoa\n","   Pope_Paul_VI\n","   George_VI\n","   Electric_motor\n","   Switzerland\n","   Mali\n","   Nonprofit_organization\n","   Raleigh,_North_Carolina\n","   Nutrition\n","   Crimean_War\n","   Literature\n","   Avicenna\n","   Chinese_characters\n","   Bermuda\n","   Nigeria\n","   Utrecht\n","   John_von_Neumann\n","   Molotov%E2%80%93Ribbentrop_Pact\n","   Capacitor\n","   History_of_science\n","   Czech_language\n","   Digimon\n","   Glacier\n","   Planck_constant\n","   Comcast\n","   Tuberculosis\n","   Affirmative_action_in_the_United_States\n","   FA_Cup\n","   Alsace\n","   Baptists\n","   Child_labour\n","   North_Carolina\n","   Heian_period\n","   On_the_Origin_of_Species\n","   Dissolution_of_the_Soviet_Union\n","   Crucifixion_of_Jesus\n","   Miami\n","   Supreme_court\n","   Textual_criticism\n","   Gramophone_record\n","   Turner_Classic_Movies\n","   Hindu_philosophy\n","   Political_party\n","   A_cappella\n","   Dominican_Order\n","   Eton_College\n","   Cork_(city)\n","   Federalism\n","   Galicia_(Spain)\n","   Green\n","   USB\n","   Sichuan\n","   Unicode\n","   Detroit\n","   Culture\n","   Sahara\n","   Rule_of_law\n","   Tibet\n","   Exhibition_game\n","   Strasbourg\n","   Oklahoma\n","   History_of_India\n","   Gamal_Abdel_Nasser\n","   Pope_John_XXIII\n","   Time\n","   European_Central_Bank\n","   St._John%27s,_Newfoundland_and_Labrador\n","   PlayStation_3\n","   Royal_assent\n","   Group_(mathematics)\n","   Central_African_Republic\n","   Asthma\n","   LaserDisc\n","   Annelid\n","   God\n","   War_on_Terror\n","   Labour_Party_(UK)\n","   Estonia\n","   Serbo-Croatian\n","   Alaska\n","   Karl_Popper\n","   Mandolin\n","   Insect\n","   Race_(human_categorization)\n","   Paris\n","   Apollo\n","   United_States_presidential_election,_2004\n","   IBM\n","   Liberal_Party_of_Australia\n","   Samurai\n","   Software_testing\n","   Glass\n","   Renewable_energy_commercialization\n","   Palermo\n","   Zinc\n","   Neoclassical_architecture\n","   CBC_Television\n","   Appalachian_Mountains\n","   Energy\n","   East_Prussia\n","   Ottoman_Empire\n","   Philosophy_of_space_and_time\n","   Neolithic\n","   Friedrich_Hayek\n","   Diarrhea\n","   Madrasa\n","   Philadelphia\n","   John_Kerry\n","   Rajasthan\n","   Guam\n","   Empiricism\n","   Idealism\n","   Education\n","   Tennessee\n","   Post-punk\n","   Canadian_football\n","   Seven_Years%27_War\n","   Richard_Feynman\n","   Muammar_Gaddafi\n","   Cyprus\n","   Steven_Spielberg\n","   Elevator\n","   Neptune\n","   Railway_electrification_system\n","   Spanish_language_in_the_United_States\n","   Charleston,_South_Carolina\n","   Red\n","   The_Blitz\n","   Endangered_Species_Act\n","   Vacuum\n","   Han_dynasty\n","   Greeks\n","   Quran\n","   Great_power\n","   Geography_of_the_United_States\n","   Compact_disc\n","   Transistor\n","   Modern_history\n","   51st_state\n","   Antenna_(radio)\n","   Flowering_plant\n","   Hyderabad\n","   Santa_Monica,_California\n","   Washington_University_in_St._Louis\n","   Central_Intelligence_Agency\n","   Pain\n","   Database\n","   Tucson,_Arizona\n","   Armenia\n","   Bacteria\n","   Printed_circuit_board\n","   Premier_League\n","   Roman_Republic\n","   Pacific_War\n","   Richmond,_Virginia\n","   San_Diego\n","   Muslim_world\n","   Iran\n","   British_Isles\n","   Association_football\n","   Georgian_architecture\n","   Liberia\n","   Windows_8\n","   Swaziland\n","   Translation\n","   Airport\n","   Kievan_Rus%27\n","   Super_Nintendo_Entertainment_System\n","   Sumer\n","   Tuvalu\n","   Immaculate_Conception\n","   Namibia\n","   Russian_language\n","   United_States_Air_Force\n","   Light-emitting_diode\n","   Bird\n","   Qing_dynasty\n","   Indigenous_peoples_of_the_Americas\n","   Egypt\n","   Mosaic\n","   University\n","   Religion_in_ancient_Rome\n","   YouTube\n","   Separation_of_church_and_state_in_the_United_States\n","   Protestantism\n","   Bras%C3%ADlia\n","   Economy_of_Greece\n","   Party_leaders_of_the_United_States_House_of_Representatives\n","   Armenians\n","   Jehovah%27s_Witnesses\n","   Dwight_D._Eisenhower\n","   The_Bronx\n","   Humanism\n","   Geological_history_of_Earth\n","   Police\n","   Punjab,_Pakistan\n","   Infection\n","   Hunting\n","   Kathmandu\n"]}],"source":["# The SQuAD dataset is stored in 'json' format. \n","# There 87,599 training samples in the dataset.\n","import json\n","\n","with open(os.path.join('./squad_dataset/train-v1.1.json'), \"r\", encoding=\"utf-8\") as reader:\n","    input_data = json.load(reader)[\"data\"]\n","\n","# List of dictionary of each row\n","examples = []\n","\n","for entry in input_data:\n","    title = entry[\"title\"] # Extract the title\n","    # print('  ', title)\n","    for paragraph in entry[\"paragraphs\"]:\n","        context_text = paragraph[\"context\"] # Extract the context\n","        for qa in paragraph[\"qas\"]:\n","            # Store Question and answer data in dictionary\n","            ex = {}\n","            ex['qas_id'] = qa[\"id\"]\n","            ex['question_text'] = qa[\"question\"]\n","            answer = qa[\"answers\"][0]\n","            ex['answer_text'] = answer[\"text\"]\n","            ex['start_position_character'] = answer[\"answer_start\"]                \n","            ex['title'] = title\n","            ex['context_text'] = context_text\n","            examples.append(ex)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cnhuQxpvZzmU","outputId":"040857d3-2266-4606-ff88-292c531fe410","executionInfo":{"status":"ok","timestamp":1660444443889,"user_tz":240,"elapsed":132,"user":{"displayName":"Soham","userId":"11550519457178793252"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["There are 87,599 training examples.\n"]}],"source":["print('There are {:,} training examples.'.format(len(examples)))"]},{"cell_type":"markdown","metadata":{"id":"VPlLOPwzDR87"},"source":["## 1.4 Inspecting Examples:"]},{"cell_type":"markdown","metadata":{"id":"W8OZ4ZWSClyd"},"source":["Each example has a **question**, and a **context**, which is the reference text in which the answer can be found. \n","\n","\n","Here are some of the field descriptions from the code:\n","* **qas_id**: The example's unique identifier\n","* **title**: Article title\n","* **question_text**: The question string\n","* **context_text**: The context string\n","* **answer_text**: The answer string\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zIC3pmmgIHjk","outputId":"60fd20c6-ccc7-476e-858e-e462cf03d2da","executionInfo":{"status":"ok","timestamp":1660444445774,"user_tz":240,"elapsed":162,"user":{"displayName":"Soham","userId":"11550519457178793252"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Title: University_of_Notre_Dame\n","ID: 5733ccbe4776f41900661271\n","\n","======== Question =========\n","In what film did a parody of the \"Win one for the Gipper\" speech appear?\n","\n","======== Context =========\n","In the film Knute Rockne, All American, Knute Rockne (played by Pat O'Brien)\n","delivers the famous \"Win one for the Gipper\" speech, at which point the\n","background music swells with the \"Notre Dame Victory March\". George Gipp was\n","played by Ronald Reagan, whose nickname \"The Gipper\" was derived from this role.\n","This scene was parodied in the movie Airplane! with the same background music,\n","only this time honoring George Zipp, one of Ted Striker's former comrades. The\n","song also was prominent in the movie Rudy, with Sean Astin as Daniel \"Rudy\"\n","Ruettiger, who harbored dreams of playing football at the University of Notre\n","Dame despite significant obstacles.\n","\n","======== Answer =========\n","Airplane!\n"]}],"source":["import textwrap\n","\n","wrapper = textwrap.TextWrapper(width=80) \n","ex = examples[260]\n","print('Title:', ex['title'])\n","print('ID:', ex['qas_id'])\n","\n","print('\\n======== Question =========')\n","print(ex['question_text'])\n","\n","print('\\n======== Context =========')\n","print(wrapper.fill(ex['context_text']))\n","\n","print('\\n======== Answer =========')\n","print(ex['answer_text'])\n"]},{"cell_type":"markdown","metadata":{"id":"csoyhyZzSSg_"},"source":["## 1.5 Helper Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gpt6tR83keZD"},"outputs":[],"source":["import time\n","import datetime\n","\n","# Helper function for formatting elapsed times.\n","# Converts floating point seconds into hh:mm:ss\n","def format_time(elapsed):\n","    '''\n","    Takes a time in seconds and returns a string hh:mm:ss\n","    '''\n","    elapsed_rounded = int(round((elapsed)))\n","    return str(datetime.timedelta(seconds=elapsed_rounded))\n","\n","# Helper function to automatically pick a reasonable interval for printing out a progress update during training.\n","# For printing updates, this will choose an interval.\n","def good_update_interval(total_iters, num_desired_updates):\n","    '''\n","    Progress update interval based on the magnitude of the total iterations.\n","    Parameters:\n","      `total_iters` - The number of iterations in the for-loop.\n","      `num_desired_updates` - How many times we want to see an update over the \n","                              course of the for-loop.\n","    '''\n","    exact_interval = total_iters / num_desired_updates\n","    order_of_mag = len(str(total_iters)) - 1\n","    round_mag = order_of_mag - 1\n","    update_interval = int(round(exact_interval, -round_mag))\n","    if update_interval == 0:\n","        update_interval = 1\n","    return update_interval\n","\n","import pandas as pd\n","import csv\n","\n","# Helper function to report current GPU memory usage.\n","# Reports how much of the GPU's memory we're using.\n","def check_gpu_mem():\n","    '''\n","    Uses Nvidia's SMI tool to check the current GPU memory usage.\n","    '''\n","    buf = os.popen('nvidia-smi --query-gpu=memory.total,memory.used --format=csv')\n","    reader = csv.reader(buf, delimiter=',')\n","    df = pd.DataFrame(reader)\n","    new_header = df.iloc[0]\n","    df = df[1:]\n","    df.columns = new_header\n","    return df"]},{"cell_type":"markdown","metadata":{"id":"hk4InpYWDWWi"},"source":["# 2.Data Preprocessing\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"BzHxH9Uw2VQH"},"source":["## 2.1 Import Tokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["b1b5facf17294164bd8907839180f8fa","13d00d6311414bdb9e2c061244b3ac46","b3ad087a336e4854a359e717448dff3a","1b0420f6fd614239bc8f2ecb2d2d3edc","30a2f1083dbc4148ab064727265ac0ca","ebfb2dca24794f36adebc54bbf41f6c0","64816c27205146dda58f49922791d497","690aca14d076470a9a4a7a4d7234a0ca","e50e4f3fe5344c9db01723cf0de0ad54","abbd73fc7bff44e8b3f917f53dc57637","15beb87344d2447d99f458db0bc734fe"]},"id":"znwmOxQsl9fE","outputId":"474573b6-d18b-4fd5-8890-7aad4dac34b5","executionInfo":{"status":"ok","timestamp":1660446226468,"user_tz":240,"elapsed":388,"user":{"displayName":"Soham","userId":"11550519457178793252"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1b5facf17294164bd8907839180f8fa"}},"metadata":{}}],"source":["from transformers import BertTokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=False)\n","\n","# Importing the tokenizer (Secondary)\n","from transformers import DistilBertTokenizer\n","tokenizer2 = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zhrb-u7G-yzW"},"outputs":[],"source":["# Distributing Sequence Length \n","# Choosing max_len\n","max_len = 384"]},{"cell_type":"markdown","metadata":{"id":"u0GHfndc7FlH"},"source":["## 2.2 Tokenizing the training set"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_ucdyc5z7TmO","outputId":"531cd240-22b3-47aa-c4e2-6c2db22b676a","executionInfo":{"status":"ok","timestamp":1660444721548,"user_tz":240,"elapsed":260621,"user":{"displayName":"Soham","userId":"11550519457178793252"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenizing 87,599 examples...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["  Example   6,000  of   87,599.    Elapsed: 0:00:17. Remaining: 0:03:58\n","  Example  12,000  of   87,599.    Elapsed: 0:00:33. Remaining: 0:03:29\n","  Example  18,000  of   87,599.    Elapsed: 0:00:49. Remaining: 0:03:08\n","  Example  24,000  of   87,599.    Elapsed: 0:01:03. Remaining: 0:02:48\n","  Example  30,000  of   87,599.    Elapsed: 0:01:20. Remaining: 0:02:34\n","  Example  36,000  of   87,599.    Elapsed: 0:01:39. Remaining: 0:02:22\n","  Example  42,000  of   87,599.    Elapsed: 0:02:00. Remaining: 0:02:10\n","  Example  48,000  of   87,599.    Elapsed: 0:02:18. Remaining: 0:01:54\n","  Example  54,000  of   87,599.    Elapsed: 0:02:36. Remaining: 0:01:37\n","  Example  60,000  of   87,599.    Elapsed: 0:02:54. Remaining: 0:01:20\n","  Example  66,000  of   87,599.    Elapsed: 0:03:14. Remaining: 0:01:03\n","  Example  72,000  of   87,599.    Elapsed: 0:03:32. Remaining: 0:00:46\n","  Example  78,000  of   87,599.    Elapsed: 0:03:50. Remaining: 0:00:28\n","  Example  84,000  of   87,599.    Elapsed: 0:04:08. Remaining: 0:00:11\n","DONE.  Tokenization took 0:04:20\n"]}],"source":["import torch\n","\n","# Time\n","t0 = time.time()\n","\n","# Lists\n","all_input_ids = []\n","attention_masks = []\n","segment_ids = [] \n","start_positions = []\n","end_positions = []\n","\n","num_dropped = 0\n","\n","# for Update-Interval\n","update_interval = good_update_interval(total_iters = len(examples), num_desired_updates = 15)\n","\n","print('Tokenizing {:,} examples...'.format(len(examples)))\n","\n","for (ex_num, ex) in enumerate(examples):\n","    # Display update information\n","    if (ex_num % update_interval) == 0 and not (ex_num == 0):\n","        elapsed = format_time(time.time() - t0)\n","        ex_per_sec = (time.time() - t0) / ex_num\n","        remaining_sec = ex_per_sec * (len(examples) - ex_num)\n","        remaining = format_time(remaining_sec)\n","        print('  Example {:>7,}  of  {:>7,}.    Elapsed: {:}. Remaining: {:}'.format(ex_num, len(examples), elapsed, remaining))\n","    \n","    answer_tokens = tokenizer.tokenize(ex['answer_text']) # Tokenize the answer\n","    sentinel_str = ' '.join(['[MASK]']*len(answer_tokens)) # \"[MASK] [MASK] [MASK] [MASK] [MASK]\"\n","    start_char_i = ex['start_position_character']\n","    end_char_i = start_char_i + len(ex['answer_text']) # Compute position of end character\n","    context_w_sentinel = ex['context_text'][:start_char_i] + sentinel_str + ex['context_text'][end_char_i:] # context-string with sentinel_str in position of answer\n","    \n","    # Returns a dictionary containing the encoded sequence or sequence pair and additional information: the mask for sequence classification and the overflowing elements if a max_length is specified.\n","    encoded_dict = tokenizer.encode_plus(\n","        ex['question_text'], \n","        context_w_sentinel,\n","        add_special_tokens = True,\n","        max_length = max_len,\n","        pad_to_max_length = True,\n","        truncation = True,\n","        return_attention_mask = True,\n","        return_tensors = 'pt')\n","    \n","    # They are token indices, numerical representations of tokens building the sequences that will be used as input by the model.\n","    input_ids = encoded_dict['input_ids']\n","\n","    # A special token representing a masked token (used by masked-language modeling pretraining objectives, like BERT).\n","    is_mask_token = (input_ids[0] == tokenizer.mask_token_id)\n","    \n","    mask_token_indices = is_mask_token.nonzero(as_tuple=False)[:, 0]\n","    if not len(mask_token_indices) == len(answer_tokens):\n","        num_dropped += 1\n","        continue\n","    \n","    start_index = mask_token_indices[0]\n","    end_index = mask_token_indices[-1]\n","    \n","    # Converts a string to a sequence of ids (integer), using the tokenizer and vocabulary.\n","    # Returns The tokenized ids of the text.\n","    answer_token_ids = tokenizer.encode(answer_tokens, \n","                                        add_special_tokens=False, \n","                                        return_tensors='pt') # Return Pytorch model\n","    \n","\n","    input_ids[0, start_index : end_index + 1] = answer_token_ids\n","    \n","    all_input_ids.append(input_ids)\n","    attention_masks.append(encoded_dict['attention_mask'])    \n","    segment_ids.append(encoded_dict['token_type_ids'])\n","    start_positions.append(start_index)\n","    end_positions.append(end_index)\n","\n","# Concatenates the given sequence of seq tensors in the given dimension. All tensors must either have the same shape (except in the concatenating dimension) or be empty.\n","all_input_ids = torch.cat(all_input_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","segment_ids = torch.cat(segment_ids, dim=0)\n","# Constructs a tensor with no autograd history by copying data\n","start_positions = torch.tensor(start_positions)\n","end_positions = torch.tensor(end_positions)\n","\n","print('DONE.  Tokenization took {:}'.format(format_time(time.time() - t0)))"]},{"cell_type":"markdown","metadata":{"id":"Wl12-BD637Ty"},"source":["# 3.Fine-Tuning BERT"]},{"cell_type":"markdown","metadata":{"id":"Lw6mvOX8ELxB"},"source":["## 3.1 Loading Initial Weights"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gFsCTp_mporB","colab":{"base_uri":"https://localhost:8080/","height":191,"referenced_widgets":["098e01e169884a9f8b415fd62dc5e5d9","07969899455f45a59350f62fc7d43dff","ddc72fb7adb74abda2552f35674631c5","67702146eee74e3886cb7d5df55a35d3","ed4d909af4ca47eea7f543acea42e084","51ea0c22bf6b46339bcf68486fdbfa6f","f6d8367e2daf4711948ee5fdeeeb4674","cc011dcd1de14596aa69c570ad4e622c","3a67a72bfa5c48a58b51ee47116671e9","ff214aa685ca49c1b43ae638bbf9dffb","b7f62d67e469462da5c1b1da1afba515","d4d92427b528466bb0957545c32c569c","9c2f5ce5841d448780d0f5d552bbae24","d84f670df4294a218c888017786cfc54","609e2d216a7b48c5bb4d1d51edc56c34","1e8399f7777d4edea77e6f8b2aa9d7cb","dfc1dcd8e68e42508a4cf7b6a92b9a6d","5b5767b21b514bf89b48085d886883d6","58953a4ea8494f67a041be503f7e9fe9","f4cdc1007dc047f6a70f62b7e0dd62f1","62fed0cef4bd4146b560393293058f20","534b24d5d00a42789a433f41c0c2fcfa"]},"outputId":"0cf62936-1e81-4707-b7eb-e1afea03b741","executionInfo":{"status":"ok","timestamp":1660444733101,"user_tz":240,"elapsed":11561,"user":{"displayName":"Soham","userId":"11550519457178793252"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"098e01e169884a9f8b415fd62dc5e5d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/436M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4d92427b528466bb0957545c32c569c"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["# The AlbertForQuestionAnswering class from the transformers library\n","from transformers import BertForQuestionAnswering, AdamW, BertConfig\n","model = BertForQuestionAnswering.from_pretrained(\"bert-base-uncased\", output_attentions = False, output_hidden_states = False)\n","\n","desc = model.cuda() # .cuda() Function Can Only Specify GPU."]},{"cell_type":"markdown","metadata":{"id":"aRp4O7D295d_"},"source":["## 3.2 Sampling and Validation Set\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GEgLpFVlo1Z-","outputId":"b51c4fb7-ce1c-48cf-b273-ee9adb154e0a","executionInfo":{"status":"ok","timestamp":1660444736534,"user_tz":240,"elapsed":541,"user":{"displayName":"Soham","userId":"11550519457178793252"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset size: 87000 samples\n"]}],"source":["# Represents a Python iterable over a dataset\n","from torch.utils.data import TensorDataset # Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension.\n","import numpy as np\n","\n","subsample = True\n","if subsample:\n","  # Randomly permute a sequence\n","    all_indices = np.random.permutation(all_input_ids.shape[0])\n","    indices = all_indices[0:87000]\n","    dataset = TensorDataset(all_input_ids[indices, :], \n","                            attention_masks[indices, :], \n","                            segment_ids[indices, :], \n","                            start_positions[indices], \n","                            end_positions[indices])\n","else:\n","    dataset = TensorDataset(all_input_ids, \n","                            attention_masks, \n","                            segment_ids, \n","                            start_positions, \n","                            end_positions)\n","    \n","print('Dataset size: {:} samples'.format(len(dataset)))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7a6HUzC8wJTQ","outputId":"39cb3922-f42e-49dc-802b-fbd480438940","executionInfo":{"status":"ok","timestamp":1660444736535,"user_tz":240,"elapsed":17,"user":{"displayName":"Soham","userId":"11550519457178793252"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["85,260 training samples\n","1,740 validation samples\n"]}],"source":["#This dataset already has a train / test split, but I'm dividing this training set to use 98% for training and 2% for validation\n","from torch.utils.data import random_split\n","\n","train_size = int(0.98 * len(dataset))\n","val_size = len(dataset) - train_size\n","\n","train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n","\n","print('{:>5,} training samples'.format(train_size))\n","print('{:>5,} validation samples'.format(val_size))"]},{"cell_type":"markdown","metadata":{"id":"ejHQThm_uVnB"},"source":["## 3.3 Batch Size and DataLoaders"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XGUqOCtgqGhP","outputId":"034081d6-8e91-48b6-8034-249a3bcf793d","executionInfo":{"status":"ok","timestamp":1660444736535,"user_tz":240,"elapsed":15,"user":{"displayName":"Soham","userId":"11550519457178793252"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["7,105 training batches & 145 validation batches\n"]}],"source":["from torch.utils.data import DataLoader, RandomSampler, SubsetRandomSampler, SequentialSampler\n","import numpy.random\n","import numpy as np\n","\n","batch_size = 12 \n","train_dataloader = DataLoader(\n","            train_dataset,\n","            sampler = RandomSampler(train_dataset),\n","            batch_size = batch_size\n","        )\n","validation_dataloader = DataLoader(\n","            val_dataset,\n","            sampler = SequentialSampler(val_dataset),\n","            batch_size = batch_size\n","        )\n","print('{:,} training batches & {:,} validation batches'.format(len(train_dataloader), len(validation_dataloader)))"]},{"cell_type":"markdown","metadata":{"id":"qRWT-D4U_Pvx"},"source":["**Optimizer:**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GLs72DuMODJO"},"outputs":[],"source":["# Optimizer with fine-tuning recommended\n","optimizer = AdamW(model.parameters(), lr = 3e-5,\n","                  eps = 1e-8\n","                  )"]},{"cell_type":"markdown","metadata":{"id":"_iaG0A5quuqz"},"source":["## 3.4 Epochs and Learning Rate Scheduler"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-p0upAhhRiIx"},"outputs":[],"source":["from transformers import get_linear_schedule_with_warmup\n","\n","epochs = 2\n","total_steps = len(train_dataloader) * epochs\n","scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                            num_warmup_steps = 0, # Default value in run_glue.py\n","                                            num_training_steps = total_steps)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s-m54ne8uMmi","outputId":"9a2c565e-d4dd-48b1-869a-12bb0768c4c9","executionInfo":{"status":"ok","timestamp":1660444736537,"user_tz":240,"elapsed":11,"user":{"displayName":"Soham","userId":"11550519457178793252"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Total number of steps: 14210\n"]}],"source":["print('Total number of steps: {}'.format(total_steps))"]},{"cell_type":"markdown","metadata":{"id":"RqfmWwUR_Sox"},"source":["## 3.5 Training Loop"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WiDKq4cLQG6H","outputId":"9669f9e5-5662-44df-9fc8-abc1c2fc22c8"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","======== Epoch 1 / 2 ========\n","Training 7,105 batches...\n","  Batch     500  of    7,105.    Elapsed: 0:07:49. Remaining: 1:43:14\n","  Batch   1,000  of    7,105.    Elapsed: 0:15:50. Remaining: 1:36:38\n","  Batch   1,500  of    7,105.    Elapsed: 0:23:51. Remaining: 1:29:06\n","  Batch   2,000  of    7,105.    Elapsed: 0:31:51. Remaining: 1:21:18\n","  Batch   2,500  of    7,105.    Elapsed: 0:39:52. Remaining: 1:13:26\n","  Batch   3,000  of    7,105.    Elapsed: 0:47:52. Remaining: 1:05:30\n","  Batch   3,500  of    7,105.    Elapsed: 0:55:53. Remaining: 0:57:33\n","  Batch   4,000  of    7,105.    Elapsed: 1:03:53. Remaining: 0:49:36\n","  Batch   4,500  of    7,105.    Elapsed: 1:11:54. Remaining: 0:41:37\n","  Batch   5,000  of    7,105.    Elapsed: 1:19:54. Remaining: 0:33:38\n","  Batch   5,500  of    7,105.    Elapsed: 1:27:55. Remaining: 0:25:39\n","  Batch   6,000  of    7,105.    Elapsed: 1:35:55. Remaining: 0:17:40\n","  Batch   6,500  of    7,105.    Elapsed: 1:43:56. Remaining: 0:09:40\n","  Batch   7,000  of    7,105.    Elapsed: 1:51:57. Remaining: 0:01:41\n","\n","  Average training loss: 1.23\n","  Training epoch took: 1:53:38\n","\n","Running Validation...\n","  Accuracy: 0.72\n","  Validation Loss: 1.01\n","  Validation took: 0:00:47\n","\n","======== Epoch 2 / 2 ========\n","Training 7,105 batches...\n","  Batch     500  of    7,105.    Elapsed: 0:08:01. Remaining: 1:45:57\n","  Batch   1,000  of    7,105.    Elapsed: 0:16:02. Remaining: 1:37:51\n","  Batch   1,500  of    7,105.    Elapsed: 0:24:02. Remaining: 1:29:50\n","  Batch   2,000  of    7,105.    Elapsed: 0:32:03. Remaining: 1:21:49\n","  Batch   2,500  of    7,105.    Elapsed: 0:40:04. Remaining: 1:13:49\n","  Batch   3,000  of    7,105.    Elapsed: 0:48:05. Remaining: 1:05:48\n","  Batch   3,500  of    7,105.    Elapsed: 0:56:06. Remaining: 0:57:47\n","  Batch   4,000  of    7,105.    Elapsed: 1:04:06. Remaining: 0:49:46\n","  Batch   4,500  of    7,105.    Elapsed: 1:12:07. Remaining: 0:41:45\n","  Batch   5,000  of    7,105.    Elapsed: 1:20:08. Remaining: 0:33:44\n","  Batch   5,500  of    7,105.    Elapsed: 1:28:09. Remaining: 0:25:43\n","  Batch   6,000  of    7,105.    Elapsed: 1:36:09. Remaining: 0:17:42\n","  Batch   6,500  of    7,105.    Elapsed: 1:44:10. Remaining: 0:09:42\n","  Batch   7,000  of    7,105.    Elapsed: 1:52:10. Remaining: 0:01:41\n","\n","  Average training loss: 0.69\n","  Training epoch took: 1:53:51\n","\n","Running Validation...\n","  Accuracy: 0.72\n","  Validation Loss: 1.01\n","  Validation took: 0:00:47\n","\n","Training complete!\n"]}],"source":["import random\n","import numpy as np\n","\n","seed_val = 42\n","\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","training_stats = []\n","\n","for epoch_i in range(0, epochs):\n","\n","    print(\"\")\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    print('Training {:,} batches...'.format(len(train_dataloader)))\n","\n","    t0 = time.time()\n","    total_train_loss = 0\n","    model.train()\n","\n","    # Setup the update interval\n","    update_interval = good_update_interval(\n","                total_iters = len(train_dataloader), \n","                num_desired_updates = 15\n","            )\n","\n","    num_batches = len(train_dataloader)\n","\n","    # iterate through each batch\n","    for step, batch in enumerate(train_dataloader):\n","        # Display the update interval\n","        if step % update_interval == 0 and not step == 0:\n","            elapsed = format_time(time.time() - t0)\n","            step_per_sec = (time.time() - t0) / step\n","            remaining_sec = step_per_sec * (num_batches - step)\n","            remaining = format_time(remaining_sec)\n","            print('  Batch {:>7,}  of  {:>7,}.    Elapsed: {:}. Remaining: {:}'.format(step, num_batches, elapsed, remaining))\n","\n","        # moves the model to the device\n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_seg_ids = batch[2].to(device)\n","        b_start_pos = batch[3].to(device)\n","        b_end_pos = batch[4].to(device)\n","\n","        # Sets the gradients of all optimized torch.Tensor s to zero\n","        model.zero_grad()\n","\n","        # Ouput\n","        outputs = model(b_input_ids, \n","                        attention_mask=b_input_mask, \n","                        token_type_ids = b_seg_ids,\n","                        start_positions=b_start_pos,\n","                        end_positions=b_end_pos)\n","       \n","        # Output Tuple ( Total span extraction loss is the sum of a Cross-Entropy for the start and end positions, Span-start scores (before SoftMax) , Span-end scores (before SoftMax))\n","        (loss, start_logits, end_logits) = outputs\n","\n","        total_train_loss += loss.item() # Returns the value of this tensor as a standard Python number. This only works for tensors with one element. For other cases, see tolist().\n","        loss.backward() # Computes the gradient of current tensor w.r.t. graph leaves.\n","        \n","        # Clips gradient norm of an iterable of parameters.\n","        # The norm is computed over all gradients together, as if they were concatenated into a single vector. Gradients are modified in-place.\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) \n","\n","        optimizer.step() # method that updates the parameters\n","        scheduler.step()\n","    \n","    # END OF INNER FOR LOOP .........................................................................................\n","\n","\n","    avg_train_loss = total_train_loss / len(train_dataloader)            \n","    \n","    training_time = format_time(time.time() - t0)\n","\n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","    print(\"  Training epoch took: {:}\".format(training_time))\n","        \n","    print(\"\")\n","    print(\"Running Validation...\")\n","\n","    # In addition, the common practice for evaluating/validation is using torch.no_grad() in pair with model.eval() to turn off gradients computation\n","    model.eval()\n","\n","    total_eval_accuracy = 0\n","    total_eval_loss = 0\n","\n","    t0_val = time.time()\n","    pred_start, pred_end, true_start, true_end = [], [], [], []\n","\n","    # Compute Validation Metrics\n","    for batch in validation_dataloader:\n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_seg_ids = batch[2].to(device)\n","        b_start_pos = batch[3].to(device)\n","        b_end_pos = batch[4].to(device)\n","        with torch.no_grad():\n","            outputs = model(b_input_ids, \n","                            token_type_ids=b_seg_ids, \n","                            attention_mask=b_input_mask,\n","                            start_positions=b_start_pos,\n","                            end_positions=b_end_pos)\n","\n","        (loss, start_logits, end_logits) = outputs        \n","\n","        total_eval_loss += loss.item()\n","        start_logits = start_logits.detach().cpu().numpy()\n","        end_logits = end_logits.detach().cpu().numpy()\n","      \n","        b_start_pos = b_start_pos.to('cpu').numpy()\n","        b_end_pos = b_end_pos.to('cpu').numpy()\n","\n","        answer_start = np.argmax(start_logits, axis=1)\n","        answer_end = np.argmax(end_logits, axis=1)\n","\n","        pred_start.append(answer_start)\n","        pred_end.append(answer_end)\n","        true_start.append(b_start_pos)\n","        true_end.append(b_end_pos)\n","\n","    pred_start = np.concatenate(pred_start, axis=0)\n","    pred_end = np.concatenate(pred_end, axis=0)\n","    true_start = np.concatenate(true_start, axis=0)\n","    true_end = np.concatenate(true_end, axis=0)\n","\n","    num_start_correct = np.sum(pred_start == true_start)\n","    num_end_correct = np.sum(pred_end == true_end)\n","\n","    total_correct = num_start_correct + num_end_correct\n","    total_indices = len(true_start) + len(true_end)\n","\n","    avg_val_accuracy = float(total_correct) / float(total_indices)\n","    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n","\n","    avg_val_loss = total_eval_loss / len(validation_dataloader)\n","    \n","    validation_time = format_time(time.time() - t0_val)\n","    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n","    print(\"  Validation took: {:}\".format(validation_time))\n","\n","    training_stats.append(\n","        {\n","            'epoch': epoch_i + 1,\n","            'Training Loss': avg_train_loss,\n","            'Valid. Loss': avg_val_loss,\n","            'Valid. Accur.': avg_val_accuracy,\n","            'Training Time': training_time,\n","            'Validation Time': validation_time\n","        }\n","    )\n","\n","print(\"\")\n","print(\"Training complete!\")"]},{"cell_type":"code","source":["torch.save(model.state_dict(), '/content/drive/MyDrive/bert1.pkl')"],"metadata":{"id":"ibIPeFR4hnze"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.save(model, '/content/drive/MyDrive/bert2.pkl')"],"metadata":{"id":"DypfwZxKiG14"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_save = model "],"metadata":{"id":"H6My75zok6kJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = torch.load('/content/drive/MyDrive/bert2.pkl')"],"metadata":{"id":"6pi69N5bjecj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b-M5ABsu6KX6"},"source":["# 4.Performance On Test Set"]},{"cell_type":"markdown","source":["## 4.1 Load saved and pre-tuned model"],"metadata":{"id":"8NUqX5AmJaMx"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Iw7CrBfSWuFi"},"outputs":[],"source":["from transformers import BertTokenizer, BertForQuestionAnswering\n","from transformers import DistilBertTokenizer, DistilBertForQuestionAnswering\n","\n","pre_tuned = False\n","\n","if pre_tuned:\n","    tokenizer = BertTokenizer.from_pretrained(\n","        'bert-large-uncased-whole-word-masking-finetuned-squad',\n","        do_lower_case=True\n","    )\n","\n","    model = BertForQuestionAnswering.from_pretrained(\n","        'bert-large-uncased-whole-word-masking-finetuned-squad', \n","    )\n","    desc = model.cuda()"]},{"cell_type":"markdown","metadata":{"id":"fiIjg4LEpPfe"},"source":["## 4.2 Parsing Test Set"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gS9qRVcHGsTn","outputId":"c9953c75-8622-41b5-8be7-eb0999495025","executionInfo":{"status":"ok","timestamp":1660444789717,"user_tz":240,"elapsed":371,"user":{"displayName":"Soham","userId":"11550519457178793252"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Unpacking SQuAD Examples...\n","Articles:\n","   Super_Bowl_50\n","   Warsaw\n","   Normans\n","   Nikola_Tesla\n","   Computational_complexity_theory\n","   Teacher\n","   Martin_Luther\n","   Southern_California\n","   Sky_(United_Kingdom)\n","   Victoria_(Australia)\n","   Huguenot\n","   Steam_engine\n","   Oxygen\n","   1973_oil_crisis\n","   Apollo_program\n","   European_Union_law\n","   Amazon_rainforest\n","   Ctenophora\n","   Fresno,_California\n","   Packet_switching\n","   Black_Death\n","   Geology\n","   Newcastle_upon_Tyne\n","   Victoria_and_Albert_Museum\n","   American_Broadcasting_Company\n","   Genghis_Khan\n","   Pharmacy\n","   Immune_system\n","   Civil_disobedience\n","   Construction\n","   Private_school\n","   Harvard_University\n","   Jacksonville,_Florida\n","   Economic_inequality\n","   Doctor_Who\n","   University_of_Chicago\n","   Yuan_dynasty\n","   Kenya\n","   Intergovernmental_Panel_on_Climate_Change\n","   Chloroplast\n","   Prime_number\n","   Rhine\n","   Scottish_Parliament\n","   Islamism\n","   Imperialism\n","   United_Methodist_Church\n","   French_and_Indian_War\n","   Force\n","DONE!\n"]}],"source":["# highest F1 score that BERT gets among the three is considered\n","import json\n","\n","with open(os.path.join('./squad_dataset/dev-v1.1.json'), \"r\", encoding=\"utf-8\") as reader:\n","    input_data = json.load(reader)[\"data\"]\n","\n","print_count = 0\n","print('Unpacking SQuAD Examples...')\n","\n","print('Articles:')\n","\n","examples = []\n","for entry in input_data:\n","    title = entry[\"title\"]\n","    print('  ', title)\n","    for paragraph in entry[\"paragraphs\"]:\n","        context_text = paragraph[\"context\"]\n","        for qa in paragraph[\"qas\"]:\n","            ex = {}\n","            ex['qas_id'] = qa[\"id\"]\n","            ex['question_text'] = qa[\"question\"]\n","            ex['answers'] = qa[\"answers\"]\n","            ex['title'] = title\n","            ex['context_text'] = context_text\n","            examples.append(ex)\n","print('DONE!')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P4VnY9cPGsTq","outputId":"02e01d11-aa49-4e01-df1a-2c383fdbc543","executionInfo":{"status":"ok","timestamp":1660444791413,"user_tz":240,"elapsed":233,"user":{"displayName":"Soham","userId":"11550519457178793252"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["There are 10,570 test examples.\n"]}],"source":["print('There are {:,} test examples.'.format(len(examples)))"]},{"cell_type":"markdown","metadata":{"id":"LBV0qHNaNT7R"},"source":["## 4.3 Locating Test Answers"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bCVlKb7zoRMK","outputId":"453d14b0-12fb-4194-f3bc-ef6b661fd5e1","executionInfo":{"status":"ok","timestamp":1660444909460,"user_tz":240,"elapsed":96232,"user":{"displayName":"Soham","userId":"11550519457178793252"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Processing 10,570 examples...\n","  Example   1,000  of   10,570.    Elapsed: 0:00:07. Remaining: 0:01:09\n","  Example   2,000  of   10,570.    Elapsed: 0:00:14. Remaining: 0:00:59\n","  Example   3,000  of   10,570.    Elapsed: 0:00:21. Remaining: 0:00:52\n","  Example   4,000  of   10,570.    Elapsed: 0:00:31. Remaining: 0:00:51\n","  Example   5,000  of   10,570.    Elapsed: 0:00:42. Remaining: 0:00:47\n","  Example   6,000  of   10,570.    Elapsed: 0:00:51. Remaining: 0:00:39\n","  Example   7,000  of   10,570.    Elapsed: 0:01:01. Remaining: 0:00:31\n","  Example   8,000  of   10,570.    Elapsed: 0:01:11. Remaining: 0:00:23\n","  Example   9,000  of   10,570.    Elapsed: 0:01:20. Remaining: 0:00:14\n","  Example  10,000  of   10,570.    Elapsed: 0:01:29. Remaining: 0:00:05\n","DONE.  Tokenization took 0:01:36\n"]}],"source":["# 2-pass approach\n","import time\n","import torch\n","import logging\n","\n","logging.getLogger(\"transformers.tokenization_utils_base\").setLevel(logging.ERROR)\n","\n","t0 = time.time()\n","start_positions = []\n","end_positions = []\n","num_clipped_answers = 0\n","num_impossible = 0\n","\n","update_interval = good_update_interval(\n","            total_iters = len(examples), \n","            num_desired_updates = 15\n","        )\n","\n","print('Processing {:,} examples...'.format(len(examples)))\n","\n","for (ex_num, ex) in enumerate(examples):\n","\n","    if (ex_num % update_interval) == 0 and not (ex_num == 0):\n","\n","        elapsed = format_time(time.time() - t0)\n","        ex_per_sec = (time.time() - t0) / ex_num\n","        remaining_sec = ex_per_sec * (len(examples) - ex_num)\n","        remaining = format_time(remaining_sec)\n","        print('  Example {:>7,}  of  {:>7,}.    Elapsed: {:}. Remaining: {:}'.format(ex_num, len(examples), elapsed, remaining))\n","    start_options = []\n","    end_options = []\n","\n","    encoded_stored = False\n","    for answer in ex['answers']:\n","        answer_tokens = tokenizer.tokenize(answer['text'])\n","        sentinel_str = ' '.join(['[MASK]']*len(answer_tokens))\n","        start_char_i = answer['answer_start']\n","        end_char_i = start_char_i + len(answer['text'])\n","        context_w_sentinel = ex['context_text'][:start_char_i] + \\\n","                            sentinel_str + \\\n","                            ex['context_text'][end_char_i:]\n","        input_ids = tokenizer.encode(\n","            ex['question_text'], \n","            context_w_sentinel,\n","            add_special_tokens = True, \n","            #max_length = max_len,\n","            pad_to_max_length = False,\n","            truncation = False,\n","        )\n","        mask_token_indices = np.where(np.array(input_ids) == tokenizer.mask_token_id)[0]\n","        assert(len(mask_token_indices) == len(answer_tokens))           \n","        start_index = mask_token_indices[0]\n","        end_index = mask_token_indices[-1]\n","        start_options.append(start_index)\n","        end_options.append(end_index)\n","    \n","    start_positions.append(start_options)\n","    end_positions.append(end_options)\n","\n","print('DONE.  Tokenization took {:}'.format(format_time(time.time() - t0)))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5W6fhTTMrM4-","outputId":"2c2b0636-c7e9-436e-86d6-5e105ae84037","executionInfo":{"status":"ok","timestamp":1660446322127,"user_tz":240,"elapsed":131,"user":{"displayName":"Soham","userId":"11550519457178793252"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Samples w/ all answers clipped: 31 of 10,570 (0.29%)\n","\n","    Additional clipped answers: 19 of 31,710\n"]}],"source":["num_impossible = 0\n","num_clipped = 0\n","\n","for (start_options, end_options) in zip(start_positions, end_positions):\n","\n","    is_possible = False\n","    for i in range(0, len(start_options)):\n","        if (start_options[i] < max_len) and (end_options[i] < max_len):\n","            is_possible = True\n","        if (start_options[i] > max_len) or (end_options[i] > max_len):\n","            num_clipped += 1\n","    if not is_possible:\n","        num_impossible += 1\n","\n","print('')\n","\n","print('Samples w/ all answers clipped: {:,} of {:,} ({:.2%})'.format(num_impossible, len(examples), float(num_impossible) / float(len(examples))))\n","\n","addtl_clipped = num_clipped - (num_impossible * 3)\n","total_answers = len(examples) * 3\n","print('\\n    Additional clipped answers: {:,} of {:,}'.format(addtl_clipped, total_answers))"]},{"cell_type":"markdown","metadata":{"id":"0RvMIcRhrCxS"},"source":["## 4.4 Tokenizing and Encoding the Test Samples"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UXmTMK9nqX9B","outputId":"12ff1b92-13fb-41bc-d149-18df1f0000ec","executionInfo":{"status":"ok","timestamp":1660446422225,"user_tz":240,"elapsed":67047,"user":{"displayName":"Soham","userId":"11550519457178793252"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenizing 10,570 examples...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["  Example   1,000  of   10,570.    Elapsed: 0:00:06. Remaining: 0:00:53\n","  Example   2,000  of   10,570.    Elapsed: 0:00:11. Remaining: 0:00:45\n","  Example   3,000  of   10,570.    Elapsed: 0:00:16. Remaining: 0:00:40\n","  Example   4,000  of   10,570.    Elapsed: 0:00:22. Remaining: 0:00:36\n","  Example   5,000  of   10,570.    Elapsed: 0:00:30. Remaining: 0:00:33\n","  Example   6,000  of   10,570.    Elapsed: 0:00:36. Remaining: 0:00:28\n","  Example   7,000  of   10,570.    Elapsed: 0:00:43. Remaining: 0:00:22\n","  Example   8,000  of   10,570.    Elapsed: 0:00:50. Remaining: 0:00:16\n","  Example   9,000  of   10,570.    Elapsed: 0:00:56. Remaining: 0:00:10\n","  Example  10,000  of   10,570.    Elapsed: 0:01:02. Remaining: 0:00:04\n","DONE.  Tokenization took 0:01:07\n"]}],"source":["import time\n","import torch\n","\n","t0 = time.time()\n","all_input_ids = []\n","attention_masks = []\n","segment_ids = [] \n","all_input_ids2 = []\n","attention_masks2 = []\n","segment_ids2 = []\n","update_interval = good_update_interval(\n","            total_iters = len(examples), \n","            num_desired_updates = 15\n","        )\n","\n","print('Tokenizing {:,} examples...'.format(len(examples)))\n","\n","for (ex_num, ex) in enumerate(examples):\n","\n","    if (ex_num % update_interval) == 0 and not (ex_num == 0):\n","        elapsed = format_time(time.time() - t0)\n","        ex_per_sec = (time.time() - t0) / ex_num\n","        remaining_sec = ex_per_sec * (len(examples) - ex_num)\n","        remaining = format_time(remaining_sec)\n","        print('  Example {:>7,}  of  {:>7,}.    Elapsed: {:}. Remaining: {:}'.format(ex_num, len(examples), elapsed, remaining))\n","\n","    encoded_dict = tokenizer.encode_plus(\n","        ex['question_text'], \n","        ex['context_text'],\n","        add_special_tokens = True,\n","        max_length = max_len,\n","        pad_to_max_length = True,\n","        truncation = True,\n","        return_attention_mask = True,\n","        return_tensors = 'pt',\n","    )\n","    encoded_dict2 = tokenizer2.encode_plus(\n","        ex['question_text'], \n","        ex['context_text'],\n","        add_special_tokens = True,\n","        max_length = max_len,\n","        pad_to_max_length = True,\n","        truncation = True,\n","        return_attention_mask = True,\n","        return_tensors = 'pt',\n","    )\n","\n","\n","    input_ids = encoded_dict['input_ids']\n","    input_ids2 = encoded_dict2['input_ids']\n"," \n","    all_input_ids.append(input_ids)\n","    attention_masks.append(encoded_dict['attention_mask'])    \n","    segment_ids.append(encoded_dict['token_type_ids'])\n","    all_input_ids2.append(input_ids2)\n","    attention_masks2.append(encoded_dict2['attention_mask'])    \n","    #segment_ids2.append(encoded_dict2['token_type_ids'])\n","\n","all_input_ids = torch.cat(all_input_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","segment_ids = torch.cat(segment_ids, dim=0)\n","all_input_ids2 = torch.cat(all_input_ids2, dim=0)\n","attention_masks2 = torch.cat(attention_masks2, dim=0)\n","#segment_ids2 = torch.cat(segment_ids2, dim=0)\n","\n","print('DONE.  Tokenization took {:}'.format(format_time(time.time() - t0)))"]},{"cell_type":"markdown","metadata":{"id":"RJJVmwstqzEn"},"source":["## 4.5 Evaluate On Test Set"]},{"cell_type":"code","source":["the_model = torch.load('/content/drive/MyDrive/bert2.pkl')"],"metadata":{"id":"M4OFnu2yj7-L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# model_dis = torch.load('/content/drive/MyDrive/distil_model.pkl')\n","import pickle\n","model_dis = pickle.load(open('/content/drive/MyDrive/distil_model.pkl', 'rb'))"],"metadata":{"id":"KSSRow1AO7HL"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xqNzZkUx6KYL","outputId":"549e46a7-68e8-4691-fafc-3d8754143caf","executionInfo":{"status":"ok","timestamp":1660447020425,"user_tz":240,"elapsed":406510,"user":{"displayName":"Soham","userId":"11550519457178793252"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Evaluating on 661 test batches...\n","  Batch      50  of      661.    Elapsed: 0:00:29. Remaining: 0:05:58\n","  Batch     100  of      661.    Elapsed: 0:01:01. Remaining: 0:05:40\n","  Batch     150  of      661.    Elapsed: 0:01:31. Remaining: 0:05:11\n","  Batch     200  of      661.    Elapsed: 0:02:02. Remaining: 0:04:41\n","  Batch     250  of      661.    Elapsed: 0:02:33. Remaining: 0:04:11\n","  Batch     300  of      661.    Elapsed: 0:03:04. Remaining: 0:03:41\n","  Batch     350  of      661.    Elapsed: 0:03:34. Remaining: 0:03:10\n","  Batch     400  of      661.    Elapsed: 0:04:05. Remaining: 0:02:40\n","  Batch     450  of      661.    Elapsed: 0:04:36. Remaining: 0:02:09\n","  Batch     500  of      661.    Elapsed: 0:05:07. Remaining: 0:01:39\n","  Batch     550  of      661.    Elapsed: 0:05:38. Remaining: 0:01:08\n","  Batch     600  of      661.    Elapsed: 0:06:09. Remaining: 0:00:37\n","  Batch     650  of      661.    Elapsed: 0:06:40. Remaining: 0:00:07\n","    DONE.\n","\n","Evaluation took 406 seconds.\n"]}],"source":["import time\n","import numpy as np\n","\n","model.eval()\n","model_dis.eval()\n","\n","t0 = time.time()\n","pred_start = []\n","pred_end = []\n","num_test_samples = all_input_ids.shape[0]\n","batch_size = 16\n","\n","num_batches = int(np.ceil(num_test_samples / batch_size))\n","\n","print('Evaluating on {:,} test batches...'.format(num_batches))\n","\n","batch_num = 0\n","for start_i in range(0, num_test_samples, batch_size):\n","    if ((batch_num % 50) == 0) and not (batch_num == 0):\n","      elapsed = format_time(time.time() - t0)\n","      batches_per_sec = (time.time() - t0) / batch_num\n","      remaining_sec = batches_per_sec * (num_batches - batch_num)\n","      remaining = format_time(remaining_sec)\n","      print('  Batch {:>7,}  of  {:>7,}.    Elapsed: {:}. Remaining: {:}'.format(batch_num, num_batches, elapsed, remaining))\n","\n","    end_i = min(start_i + batch_size, num_test_samples)\n","    b_input_ids = all_input_ids[start_i:end_i, :]\n","    b_attn_masks = attention_masks[start_i:end_i, :]\n","    b_seg_ids = segment_ids[start_i:end_i, :]   \n","\n","    b_input_ids2 = all_input_ids[start_i:end_i, :]\n","    b_attn_masks2 = attention_masks[start_i:end_i, :]\n","    #b_seg_ids = segment_ids[start_i:end_i, :]\n","\n","    b_input_ids = b_input_ids.to(device)\n","    b_attn_masks = b_attn_masks.to(device)\n","    b_seg_ids = b_seg_ids.to(device)\n","\n","    b_input_ids2 = b_input_ids2.to(device)\n","    b_attn_masks2 = b_attn_masks2.to(device)\n","    #b_seg_ids = b_seg_ids.to(device)\n","\n","    with torch.no_grad():\n","        (start_logits, end_logits) = model(b_input_ids, \n","                                           attention_mask=b_attn_masks,\n","                                           token_type_ids=b_seg_ids)\n","        (start_logits2, end_logits2) = model_dis(b_input_ids2, \n","                                           attention_mask=b_attn_masks2)\n","    start_logits = start_logits.detach().cpu().numpy()\n","    end_logits = end_logits.detach().cpu().numpy()\n","    start_logits2 = start_logits2.detach().cpu().numpy()\n","    end_logits2 = end_logits2.detach().cpu().numpy()\n","    \n","    start_logits_a, end_logits_a = (start_logits+end_logits)/2 , (start_logits2+end_logits2)/2\n","    \n","    answer_start = np.argmax(start_logits_a, axis=1)\n","    answer_end = np.argmax(end_logits_a, axis=1)\n","\n","    pred_start.append(answer_start)\n","    pred_end.append(answer_end)\n","\n","    batch_num += 1\n","\n","pred_start = np.concatenate(pred_start, axis=0)\n","pred_end = np.concatenate(pred_end, axis=0)\n","\n","print('    DONE.')\n","\n","print('\\nEvaluation took {:.0f} seconds.'.format(time.time() - t0))"]},{"cell_type":"markdown","metadata":{"id":"xKoBf7Tm4Ync"},"source":["#5.Results"]},{"cell_type":"markdown","metadata":{"id":"XYV1A4nIE5Q8"},"source":["Exact Match:  Number of  predicted start and end indices that are equal to the correct ones are added up for this metric"]},{"cell_type":"code","source":["total_correct = 0\n","\n","for i in range(0, len(pred_start)):\n","\n","    match_options = []\n","    for j in range (0, len(start_positions[i])):\n","        matches = 0\n","        if pred_start[i] == start_positions[i][j] or pred_start2[i] == start_positions[i][j]:\n","            matches += 1\n","        if pred_end[i] == end_positions[i][j]:\n","            matches += 1\n","\n","        match_options.append(matches)\n","\n","    total_correct += (max(match_options))\n","total_indices = len(pred_start) + len(pred_end)\n","\n","print('Correctly predicted indeces: {:,} of {:,} ({:.2%})'.format(\n","    total_correct,\n","    total_indices,\n","    float(total_correct) / float(total_indices)\n","))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nnzCzfnMKkw5","executionInfo":{"status":"ok","timestamp":1660527832865,"user_tz":240,"elapsed":3,"user":{"displayName":"Soham Shinde","userId":"16480171809645537358"}},"outputId":"1857327d-d5a0-4472-cfe5-fac917fa5ffa"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Correctly predicted indeces: 17,770 of 21,140 (84.06%)\n"]}]},{"cell_type":"markdown","metadata":{"id":"FCO7vPcP8GgE"},"source":["**F1 Score**\n","\n","precision = 1.0 * num_same / len(pred_toks)\n","\n","recall = 1.0 * num_same / len(gold_toks)\n","\n","f1 = (2 * precision * recall) / (precision + recall)\n","\n"]},{"cell_type":"code","source":["f1s = []\n","for i in range(0, len(pred_start)):\n","    pred_span = set(range(pred_start[i], pred_end[i] + 1))\n","    f1_options = []\n","    for j in range (0, len(start_positions[i])):\n","        true_span = set(range(start_positions[i][j], end_positions[i][j] + 1))    \n","        num_same = len(pred_span.intersection(true_span))\n","        if num_same == 0:\n","            f1_options.append(0)\n","            continue\n","        precision = float(num_same) / float(len(pred_span))\n","        recall = float(num_same) / float(len(true_span))\n","        f1 = (2 * precision * recall) / (precision + recall)\n","        f1_options.append(f1)\n","    f1s.append(max(f1_options))\n","\n","print('Average F1 Score: {:.3f}'.format(np.mean(f1s)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7Y1O7SKmLNaa","executionInfo":{"status":"ok","timestamp":1660527906770,"user_tz":240,"elapsed":199,"user":{"displayName":"Soham Shinde","userId":"16480171809645537358"}},"outputId":"ad33c77c-21d1-4d61-e9e5-8cfcfc181ca6"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Average F1 Score: 0.865\n"]}]},{"cell_type":"markdown","metadata":{"id":"yNryoCi9JWng"},"source":["**Final Score of our fine tuned BERT base model:**\n","\n","\n","Correctly predicted indeces: 17,770 of 21,140 (84.06%)\n","\n","Average F1 Score: 0.865\n","\n","\n"]},{"cell_type":"markdown","source":["**Score of our Pre tuned BERT base model:**\n","\n","\n","Correctly predicted indeces: 18446 of 21,140 (88.26%)\n","\n","Average F1 Score: 0.884"],"metadata":{"id":"dPp3TGrsLf-p"}}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"BERT_final(84%).ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"b1b5facf17294164bd8907839180f8fa":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_13d00d6311414bdb9e2c061244b3ac46","IPY_MODEL_b3ad087a336e4854a359e717448dff3a","IPY_MODEL_1b0420f6fd614239bc8f2ecb2d2d3edc"],"layout":"IPY_MODEL_30a2f1083dbc4148ab064727265ac0ca"}},"13d00d6311414bdb9e2c061244b3ac46":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ebfb2dca24794f36adebc54bbf41f6c0","placeholder":"​","style":"IPY_MODEL_64816c27205146dda58f49922791d497","value":"Downloading: 100%"}},"b3ad087a336e4854a359e717448dff3a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_690aca14d076470a9a4a7a4d7234a0ca","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e50e4f3fe5344c9db01723cf0de0ad54","value":231508}},"1b0420f6fd614239bc8f2ecb2d2d3edc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_abbd73fc7bff44e8b3f917f53dc57637","placeholder":"​","style":"IPY_MODEL_15beb87344d2447d99f458db0bc734fe","value":" 232k/232k [00:00&lt;00:00, 4.37MB/s]"}},"30a2f1083dbc4148ab064727265ac0ca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ebfb2dca24794f36adebc54bbf41f6c0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"64816c27205146dda58f49922791d497":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"690aca14d076470a9a4a7a4d7234a0ca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e50e4f3fe5344c9db01723cf0de0ad54":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"abbd73fc7bff44e8b3f917f53dc57637":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"15beb87344d2447d99f458db0bc734fe":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"098e01e169884a9f8b415fd62dc5e5d9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_07969899455f45a59350f62fc7d43dff","IPY_MODEL_ddc72fb7adb74abda2552f35674631c5","IPY_MODEL_67702146eee74e3886cb7d5df55a35d3"],"layout":"IPY_MODEL_ed4d909af4ca47eea7f543acea42e084"}},"07969899455f45a59350f62fc7d43dff":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_51ea0c22bf6b46339bcf68486fdbfa6f","placeholder":"​","style":"IPY_MODEL_f6d8367e2daf4711948ee5fdeeeb4674","value":"Downloading: 100%"}},"ddc72fb7adb74abda2552f35674631c5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cc011dcd1de14596aa69c570ad4e622c","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3a67a72bfa5c48a58b51ee47116671e9","value":570}},"67702146eee74e3886cb7d5df55a35d3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ff214aa685ca49c1b43ae638bbf9dffb","placeholder":"​","style":"IPY_MODEL_b7f62d67e469462da5c1b1da1afba515","value":" 570/570 [00:00&lt;00:00, 16.4kB/s]"}},"ed4d909af4ca47eea7f543acea42e084":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"51ea0c22bf6b46339bcf68486fdbfa6f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f6d8367e2daf4711948ee5fdeeeb4674":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cc011dcd1de14596aa69c570ad4e622c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3a67a72bfa5c48a58b51ee47116671e9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ff214aa685ca49c1b43ae638bbf9dffb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b7f62d67e469462da5c1b1da1afba515":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d4d92427b528466bb0957545c32c569c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9c2f5ce5841d448780d0f5d552bbae24","IPY_MODEL_d84f670df4294a218c888017786cfc54","IPY_MODEL_609e2d216a7b48c5bb4d1d51edc56c34"],"layout":"IPY_MODEL_1e8399f7777d4edea77e6f8b2aa9d7cb"}},"9c2f5ce5841d448780d0f5d552bbae24":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dfc1dcd8e68e42508a4cf7b6a92b9a6d","placeholder":"​","style":"IPY_MODEL_5b5767b21b514bf89b48085d886883d6","value":"Downloading: 100%"}},"d84f670df4294a218c888017786cfc54":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_58953a4ea8494f67a041be503f7e9fe9","max":435779157,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f4cdc1007dc047f6a70f62b7e0dd62f1","value":435779157}},"609e2d216a7b48c5bb4d1d51edc56c34":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_62fed0cef4bd4146b560393293058f20","placeholder":"​","style":"IPY_MODEL_534b24d5d00a42789a433f41c0c2fcfa","value":" 436M/436M [00:08&lt;00:00, 63.4MB/s]"}},"1e8399f7777d4edea77e6f8b2aa9d7cb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dfc1dcd8e68e42508a4cf7b6a92b9a6d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5b5767b21b514bf89b48085d886883d6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"58953a4ea8494f67a041be503f7e9fe9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f4cdc1007dc047f6a70f62b7e0dd62f1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"62fed0cef4bd4146b560393293058f20":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"534b24d5d00a42789a433f41c0c2fcfa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}